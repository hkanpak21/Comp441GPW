{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Watermarking Methods - Complete Experiments\n",
                "\n",
                "This notebook implements and evaluates watermarking methods:\n",
                "\n",
                "**Prior Work:**\n",
                "1. **Unigram-Watermark** (Zhao et al., 2023) - Provable Robust Watermarking\n",
                "2. **KGW/TGRL** (Kirchenbauer et al., 2023) - Red-Green List Watermarking\n",
                "3. **SEMSTAMP** (Hou et al., 2024) - Semantic Watermark with LSH\n",
                "\n",
                "**Our Methods:**\n",
                "4. **GPW** - Gaussian Pancakes Watermarking (basic)\n",
                "5. **GPW-SP** - GPW with Salted Phase\n",
                "6. **GPW-SP+SR** - GPW with Salted Phase + Semantic Representation Coupling\n",
                "\n",
                "## Table of Contents\n",
                "1. [Setup & Installation](#setup)\n",
                "2. [Load Models and Data](#load)\n",
                "3. [Watermark Generation](#generation)\n",
                "4. [Detection Experiments](#detection)\n",
                "5. [Attack Robustness](#attacks)\n",
                "6. [Quality Evaluation](#quality)\n",
                "7. [Comparison & Analysis](#comparison)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Unzip uploaded workspace and move to contents/\n",
                "# Upload watermark_experiments.zip to Colab first, then run this cell\n",
                "%%bash\n",
                "echo \"Looking for zip file...\"\n",
                "ZIP_FILE=$(find . -maxdepth 1 -name \"*.zip\" -type f | head -n 1)\n",
                "if [ -n \"$ZIP_FILE\" ]; then\n",
                "  echo \"Found zip file: $ZIP_FILE\"\n",
                "  echo \"Extracting to WATERMARK_EXPERIMENTS/...\"\n",
                "  unzip -q \"$ZIP_FILE\" -d WATERMARK_EXPERIMENTS\n",
                "  echo \"Moving contents from WATERMARK_EXPERIMENTS/watermark_experiments/ to current directory...\"\n",
                "  if [ -d \"WATERMARK_EXPERIMENTS/watermark_experiments\" ]; then\n",
                "    cp -r WATERMARK_EXPERIMENTS/watermark_experiments/* .\n",
                "  else\n",
                "    cp -r WATERMARK_EXPERIMENTS/* .\n",
                "  fi\n",
                "  echo \"Cleaning up...\"\n",
                "  rm -rf WATERMARK_EXPERIMENTS\n",
                "  echo \"Done! Workspace contents are now in the current directory.\"\n",
                "else\n",
                "  echo \"No zip file found. Please upload watermark_experiments.zip first.\"\n",
                "fi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Installation <a name=\"setup\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m122.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25h"
                    ]
                }
            ],
            "source": [
                "# Install required packages (run once)\n",
                "!pip install -q torch transformers accelerate\n",
                "!pip install -q sentence-transformers nltk\n",
                "!pip install -q bert-score mauve-text\n",
                "!pip install -q datasets scikit-learn scipy\n",
                "!pip install -q openai  # For GPT-based attacks (optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# This cell intentionally left blank\n",
                "# All setup is done in the installation cell above"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Set device\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# For reproducibility\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'watermarkers'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-3296306619.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import our watermarking framework\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This notebook is inside watermark_experiments/ folder, so we use direct imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m from watermarkers import (\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mUnigramWatermark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKGWWatermark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEMSTAMPWatermark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mGPWWatermark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPWConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSRConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_gpw_variant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'watermarkers'",
                        "",
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "# Import our watermarking framework\n",
                "# This notebook is inside watermark_experiments/ folder, so we use direct imports\n",
                "from watermarkers import (\n",
                "    UnigramWatermark, KGWWatermark, SEMSTAMPWatermark,\n",
                "    GPWWatermark, GPWConfig, SRConfig, create_gpw_variant\n",
                ")\n",
                "from attacks import (\n",
                "    SynonymAttack, SwapAttack, TypoAttack,\n",
                "    PegasusAttack, BigramAttack, CopyPasteAttack\n",
                ")\n",
                "from metrics.detection import compute_detection_metrics, tpr_at_fpr\n",
                "from metrics.quality import compute_perplexity, compute_bertscore, compute_diversity\n",
                "# Note: C4 data loading is done manually in the data loading cell below\n",
                "# We don't import from data_loaders to avoid dependency conflicts"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Models and Data <a name=\"load\"></a>\n",
                "\n",
                "### Model Selection\n",
                "\n",
                "We use `Qwen2.5-14B-Instruct` by default for state-of-the-art results.\n",
                "\n",
                "**For paper replication, you can use:**\n",
                "- `facebook/opt-1.3b` (KGW, SEMSTAMP papers)\n",
                "- `openai-community/gpt2-xl` (Unigram paper)\n",
                "- `gpt2` (for quick testing)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "# Choose model (uncomment one)\n",
                "# MODEL_NAME = \"gpt2\"  # Quick testing\n",
                "MODEL_NAME = \"facebook/opt-1.3b\"  # Original paper model\n",
                "# MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"  # Faster alternative\n",
                "# MODEL_NAME = \"Qwen/Qwen2.5-14B-Instruct\"  # Best results\n",
                "\n",
                "print(f\"Loading model: {MODEL_NAME}\")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
                "    device_map=\"auto\" if device == \"cuda\" else None,\n",
                "    trust_remote_code=True\n",
                ")\n",
                "if device == \"cpu\":\n",
                "    model = model.to(device)\n",
                "\n",
                "# Ensure pad token is set\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "print(f\"Model loaded successfully on {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load SEMSTAMP sentence encoder (for SEMSTAMP experiments)\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# ENCODER_NAME = \"all-mpnet-base-v2\"  # General use\n",
                "ENCODER_NAME = \"AbeHou/SemStamp-c4-sbert\"  # Paper fine-tuned encoder\n",
                "\n",
                "print(f\"Loading sentence encoder: {ENCODER_NAME}\")\n",
                "sentence_encoder = SentenceTransformer(ENCODER_NAME, device=device)\n",
                "print(\"Encoder loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load C4 dataset manually (without using data_loaders)\n",
                "print(\"Loading C4 RealNewsLike dataset...\")\n",
                "print(\"This may take a few minutes for the first download...\")\n",
                "\n",
                "from datasets import load_dataset\n",
                "import random\n",
                "\n",
                "random.seed(42)\n",
                "\n",
                "# Load C4 dataset with streaming\n",
                "try:\n",
                "    print(\"Attempting to load C4 realnewslike dataset...\")\n",
                "    dataset = load_dataset(\"c4\", \"realnewslike\", split=\"validation\", streaming=True)\n",
                "    print(\"Dataset loaded successfully!\")\n",
                "except Exception as e:\n",
                "    print(f\"Error loading with 'c4' name: {e}\")\n",
                "    print(\"Trying alternative: allenai/c4...\")\n",
                "    try:\n",
                "        dataset = load_dataset(\"allenai/c4\", \"realnewslike\", split=\"validation\", streaming=True)\n",
                "        print(\"Dataset loaded successfully with allenai/c4!\")\n",
                "    except Exception as e2:\n",
                "        raise RuntimeError(f\"Failed to load C4 dataset. Errors: {e}, {e2}\") from e2\n",
                "\n",
                "# Collect samples\n",
                "num_samples = 200\n",
                "min_length = 100\n",
                "max_length = 1000\n",
                "\n",
                "c4_data = []\n",
                "print(f\"Collecting {num_samples} samples...\")\n",
                "\n",
                "for item in dataset:\n",
                "    text = item.get(\"text\", \"\")\n",
                "    \n",
                "    # Filter by length\n",
                "    if len(text) < min_length or len(text) > max_length:\n",
                "        continue\n",
                "    \n",
                "    # Create prompt from first 30 words\n",
                "    words = text.split()\n",
                "    if len(words) < 10:\n",
                "        continue\n",
                "    \n",
                "    prompt_words = words[:30]\n",
                "    prompt = \" \".join(prompt_words)\n",
                "    \n",
                "    c4_data.append({\n",
                "        \"text\": text,\n",
                "        \"prompt\": prompt,\n",
                "        \"source\": \"c4-realnewslike\"\n",
                "    })\n",
                "    \n",
                "    if len(c4_data) >= num_samples:\n",
                "        break\n",
                "\n",
                "print(f\"\\nLoaded {len(c4_data)} C4 samples\")\n",
                "if len(c4_data) > 0:\n",
                "    print(f\"\\nExample prompt: {c4_data[0]['prompt'][:100]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split data into prompts and human baselines\n",
                "prompts = [d[\"prompt\"] for d in c4_data[:100]]\n",
                "human_texts = [d[\"text\"] for d in c4_data[100:200]]\n",
                "\n",
                "print(f\"Prompts: {len(prompts)}, Human texts: {len(human_texts)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Watermark Generation <a name=\"generation\"></a>\n",
                "\n",
                "Initialize all watermarking methods (3 prior work + 3 GPW variants) and generate watermarked text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize PRIOR WORK watermarkers\n",
                "\n",
                "# Unigram-Watermark (Zhao et al., 2023)\n",
                "unigram_wm = UnigramWatermark(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    gamma=0.5,\n",
                "    delta=2.0,\n",
                "    z_threshold=4.0,\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# KGW Watermark (Kirchenbauer et al., 2023)\n",
                "kgw_wm = KGWWatermark(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    gamma=0.5,\n",
                "    delta=2.0,\n",
                "    z_threshold=4.0,\n",
                "    context_width=1,\n",
                "    seeding_scheme=\"simple_1\",\n",
                "    ignore_repeated_bigrams=True,\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# SEMSTAMP (Hou et al., 2024)\n",
                "semstamp_wm = SEMSTAMPWatermark(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    embedder=sentence_encoder,\n",
                "    lsh_dim=3,\n",
                "    margin=0.02,\n",
                "    z_threshold=4.0,\n",
                "    device=device\n",
                ")\n",
                "\n",
                "print(\"Prior work watermarkers initialized!\")\n",
                "print(f\"  Unigram: {unigram_wm.get_config()}\")\n",
                "print(f\"  KGW: {kgw_wm.get_config()}\")\n",
                "print(f\"  SEMSTAMP: {semstamp_wm.get_config()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize OUR GPW watermarkers (3 variants)\n",
                "\n",
                "# GPW - Basic (no salted phase, no SR)\n",
                "gpw_basic = create_gpw_variant(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    variant=\"GPW\",\n",
                "    alpha=1.2,\n",
                "    omega=10.0,\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# GPW-SP - Salted Phase\n",
                "gpw_sp = create_gpw_variant(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    variant=\"GPW-SP\",\n",
                "    alpha=1.2,\n",
                "    omega=10.0,\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# GPW-SP+SR - Salted Phase + Semantic Representation Coupling\n",
                "gpw_sp_sr = create_gpw_variant(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    variant=\"GPW-SP+SR\",\n",
                "    alpha=1.2,\n",
                "    omega=10.0,\n",
                "    device=device\n",
                ")\n",
                "\n",
                "print(\"\\nGPW watermarkers initialized!\")\n",
                "print(f\"  GPW: {gpw_basic.get_config()}\")\n",
                "print(f\"  GPW-SP: {gpw_sp.get_config()}\")\n",
                "print(f\"  GPW-SP+SR: {gpw_sp_sr.get_config()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# All watermarkers dict for iteration\n",
                "ALL_WATERMARKERS = {\n",
                "    # Prior work\n",
                "    \"Unigram\": unigram_wm,\n",
                "    \"KGW\": kgw_wm,\n",
                "    \"SEMSTAMP\": semstamp_wm,\n",
                "    # Our methods\n",
                "    \"GPW\": gpw_basic,\n",
                "    \"GPW-SP\": gpw_sp,\n",
                "    \"GPW-SP+SR\": gpw_sp_sr,\n",
                "}\n",
                "\n",
                "print(f\"Total watermarkers: {len(ALL_WATERMARKERS)}\")\n",
                "print(f\"Methods: {list(ALL_WATERMARKERS.keys())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate watermarked texts (this may take a while)\n",
                "NUM_SAMPLES = 50  # Adjust based on available time/compute\n",
                "\n",
                "wm_texts = {}  # Store generated texts for each method\n",
                "\n",
                "for method_name, watermarker in ALL_WATERMARKERS.items():\n",
                "    print(f\"\\nGenerating {method_name} watermarked texts...\")\n",
                "    wm_texts[method_name] = []\n",
                "    for prompt in tqdm(prompts[:NUM_SAMPLES]):\n",
                "        try:\n",
                "            text = watermarker.generate(prompt, max_new_tokens=120, temperature=0.9, top_p=0.95)\n",
                "            wm_texts[method_name].append(text)\n",
                "        except Exception as e:\n",
                "            print(f\"Error with {method_name}: {e}\")\n",
                "            wm_texts[method_name].append(prompt)  # Fallback\n",
                "\n",
                "print(f\"\\n\\nGeneration complete!\")\n",
                "for name, texts in wm_texts.items():\n",
                "    print(f\"  {name}: {len(texts)} texts\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display sample generated texts\n",
                "print(\"=\" * 80)\n",
                "print(\"SAMPLE GENERATED TEXTS\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for method_name in ALL_WATERMARKERS.keys():\n",
                "    print(f\"\\n[{method_name}]\\n{wm_texts[method_name][0][:200]}...\")\n",
                "\n",
                "print(f\"\\n[Human Text]\\n{human_texts[0][:200]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Detection Experiments <a name=\"detection\"></a>\n",
                "\n",
                "Evaluate detection accuracy for each watermarking method."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detect watermarks in all texts\n",
                "wm_scores = {}  # Watermarked text scores\n",
                "human_scores = {}  # Human text scores (for each detector)\n",
                "\n",
                "for method_name, watermarker in ALL_WATERMARKERS.items():\n",
                "    print(f\"\\nRunning {method_name} detection...\")\n",
                "    \n",
                "    # Detect in watermarked texts\n",
                "    wm_scores[method_name] = []\n",
                "    for t in tqdm(wm_texts[method_name], desc=\"WM texts\"):\n",
                "        try:\n",
                "            result = watermarker.detect(t)\n",
                "            wm_scores[method_name].append(result.z_score)\n",
                "        except Exception as e:\n",
                "            wm_scores[method_name].append(0.0)\n",
                "    \n",
                "    # Detect in human texts (for FPR calibration)\n",
                "    human_scores[method_name] = []\n",
                "    for t in tqdm(human_texts[:NUM_SAMPLES], desc=\"Human texts\"):\n",
                "        try:\n",
                "            result = watermarker.detect(t)\n",
                "            human_scores[method_name].append(result.z_score)\n",
                "        except Exception as e:\n",
                "            human_scores[method_name].append(0.0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute detection metrics for all methods\n",
                "all_metrics = {}\n",
                "\n",
                "for method_name in ALL_WATERMARKERS.keys():\n",
                "    all_metrics[method_name] = compute_detection_metrics(\n",
                "        wm_scores[method_name], \n",
                "        human_scores[method_name]\n",
                "    )\n",
                "\n",
                "# Create results table\n",
                "results_data = []\n",
                "for method_name, metrics in all_metrics.items():\n",
                "    results_data.append({\n",
                "        \"Method\": method_name,\n",
                "        \"AUC\": metrics[\"auc\"],\n",
                "        \"TPR@FPR=1%\": metrics[\"tpr_at_fpr_1\"],\n",
                "        \"TPR@FPR=5%\": metrics[\"tpr_at_fpr_5\"],\n",
                "        \"Mean WM Z-score\": metrics[\"mean_wm_score\"],\n",
                "        \"Mean Human Z-score\": metrics[\"mean_human_score\"],\n",
                "    })\n",
                "\n",
                "results_df = pd.DataFrame(results_data)\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"DETECTION RESULTS (No Attack)\")\n",
                "print(\"=\" * 80)\n",
                "print(results_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot Z-score distributions for all 6 methods\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for idx, method_name in enumerate(ALL_WATERMARKERS.keys()):\n",
                "    ax = axes[idx]\n",
                "    ax.hist(human_scores[method_name], bins=20, alpha=0.6, label=\"Human\", color=\"blue\", density=True)\n",
                "    ax.hist(wm_scores[method_name], bins=20, alpha=0.6, label=\"Watermarked\", color=\"red\", density=True)\n",
                "    ax.axvline(x=4.0, color=\"black\", linestyle=\"--\", label=\"Threshold\")\n",
                "    ax.set_xlabel(\"Z-score\")\n",
                "    ax.set_ylabel(\"Density\")\n",
                "    ax.set_title(f\"{method_name}\")\n",
                "    ax.legend(fontsize=8)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"z_score_distributions_all.png\", dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot ROC curves for all methods\n",
                "from sklearn.metrics import roc_curve, auc\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "\n",
                "colors = plt.cm.tab10(np.linspace(0, 1, len(ALL_WATERMARKERS)))\n",
                "\n",
                "for idx, method_name in enumerate(ALL_WATERMARKERS.keys()):\n",
                "    scores = np.concatenate([wm_scores[method_name], human_scores[method_name]])\n",
                "    labels = np.concatenate([np.ones(len(wm_scores[method_name])), np.zeros(len(human_scores[method_name]))])\n",
                "    fpr, tpr, _ = roc_curve(labels, scores)\n",
                "    roc_auc = auc(fpr, tpr)\n",
                "    ax.plot(fpr, tpr, label=f\"{method_name} (AUC = {roc_auc:.3f})\", color=colors[idx], linewidth=2)\n",
                "\n",
                "ax.plot([0, 1], [0, 1], \"k--\", label=\"Random\", alpha=0.5)\n",
                "ax.set_xlabel(\"False Positive Rate\", fontsize=12)\n",
                "ax.set_ylabel(\"True Positive Rate\", fontsize=12)\n",
                "ax.set_title(\"ROC Curves for All Watermark Methods\", fontsize=14)\n",
                "ax.legend(loc=\"lower right\")\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.savefig(\"roc_curves_all.png\", dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Attack Robustness <a name=\"attacks\"></a>\n",
                "\n",
                "Evaluate detection accuracy after various attacks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize attacks\n",
                "attacks = {\n",
                "    \"Synonym (30%)\": SynonymAttack(edit_rate=0.3),\n",
                "    \"Swap (20%)\": SwapAttack(edit_rate=0.2),\n",
                "    \"Typo (30%)\": TypoAttack(edit_rate=0.3),\n",
                "}\n",
                "\n",
                "print(f\"Configured attacks: {list(attacks.keys())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run attacks and measure detection for all methods\n",
                "attack_results = {}\n",
                "\n",
                "for attack_name, attack in attacks.items():\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Applying {attack_name}...\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    attack_results[attack_name] = {}\n",
                "    \n",
                "    for method_name, watermarker in ALL_WATERMARKERS.items():\n",
                "        # Apply attack\n",
                "        attacked_texts = [attack(t) for t in wm_texts[method_name]]\n",
                "        \n",
                "        # Detect\n",
                "        attacked_scores = []\n",
                "        for t in attacked_texts:\n",
                "            try:\n",
                "                result = watermarker.detect(t)\n",
                "                attacked_scores.append(result.z_score)\n",
                "            except:\n",
                "                attacked_scores.append(0.0)\n",
                "        \n",
                "        # Compute metrics\n",
                "        tpr_1, _ = tpr_at_fpr(attacked_scores, human_scores[method_name], target_fpr=0.01)\n",
                "        tpr_5, _ = tpr_at_fpr(attacked_scores, human_scores[method_name], target_fpr=0.05)\n",
                "        \n",
                "        attack_results[attack_name][method_name] = {\n",
                "            \"TPR@FPR=1%\": tpr_1,\n",
                "            \"TPR@FPR=5%\": tpr_5,\n",
                "            \"Mean Z-score\": np.mean(attacked_scores),\n",
                "        }\n",
                "        \n",
                "        print(f\"  {method_name}: TPR@1%={tpr_1:.3f}, TPR@5%={tpr_5:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create attack robustness table\n",
                "robustness_data = []\n",
                "\n",
                "for attack_name, results in attack_results.items():\n",
                "    for method_name, metrics in results.items():\n",
                "        robustness_data.append({\n",
                "            \"Attack\": attack_name,\n",
                "            \"Method\": method_name,\n",
                "            \"TPR@FPR=1%\": metrics[\"TPR@FPR=1%\"],\n",
                "            \"TPR@FPR=5%\": metrics[\"TPR@FPR=5%\"],\n",
                "        })\n",
                "\n",
                "robustness_df = pd.DataFrame(robustness_data)\n",
                "\n",
                "# Pivot for better visualization\n",
                "pivot_df = robustness_df.pivot(index=\"Attack\", columns=\"Method\", values=\"TPR@FPR=1%\")\n",
                "\n",
                "# Reorder columns\n",
                "column_order = [\"Unigram\", \"KGW\", \"SEMSTAMP\", \"GPW\", \"GPW-SP\", \"GPW-SP+SR\"]\n",
                "pivot_df = pivot_df[[c for c in column_order if c in pivot_df.columns]]\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"ATTACK ROBUSTNESS (TPR@FPR=1%)\")\n",
                "print(\"=\" * 80)\n",
                "print(pivot_df.to_string())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot attack robustness comparison\n",
                "fig, ax = plt.subplots(figsize=(14, 6))\n",
                "\n",
                "pivot_df.plot(kind=\"bar\", ax=ax, width=0.8, edgecolor=\"black\")\n",
                "ax.set_xlabel(\"Attack\", fontsize=12)\n",
                "ax.set_ylabel(\"TPR @ FPR=1%\", fontsize=12)\n",
                "ax.set_title(\"Watermark Robustness to Attacks\", fontsize=14)\n",
                "ax.legend(title=\"Method\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
                "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
                "ax.set_ylim(0, 1)\n",
                "ax.axhline(y=0.5, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"attack_robustness_all.png\", dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Quality Evaluation <a name=\"quality\"></a>\n",
                "\n",
                "Evaluate text quality of watermarked outputs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute perplexity for all methods\n",
                "print(\"Computing perplexity (using GPT-2)...\")\n",
                "\n",
                "ppl_results = {}\n",
                "for method_name in ALL_WATERMARKERS.keys():\n",
                "    ppl_results[method_name] = compute_perplexity(wm_texts[method_name], model_name=\"gpt2\", device=device)\n",
                "    print(f\"  {method_name}: {ppl_results[method_name]['mean']:.2f}\")\n",
                "\n",
                "ppl_human = compute_perplexity(human_texts[:NUM_SAMPLES], model_name=\"gpt2\", device=device)\n",
                "print(f\"  Human: {ppl_human['mean']:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute diversity metrics\n",
                "print(\"\\nComputing diversity metrics...\")\n",
                "\n",
                "div_results = {}\n",
                "for method_name in ALL_WATERMARKERS.keys():\n",
                "    div_results[method_name] = compute_diversity(wm_texts[method_name])\n",
                "\n",
                "div_human = compute_diversity(human_texts[:NUM_SAMPLES])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create quality comparison table\n",
                "quality_data = [{\"Method\": \"Human\", \"Perplexity\": ppl_human[\"mean\"], \"Distinct-4\": div_human[\"distinct_4\"]}]\n",
                "\n",
                "for method_name in ALL_WATERMARKERS.keys():\n",
                "    quality_data.append({\n",
                "        \"Method\": method_name,\n",
                "        \"Perplexity\": ppl_results[method_name][\"mean\"],\n",
                "        \"Distinct-4\": div_results[method_name][\"distinct_4\"],\n",
                "    })\n",
                "\n",
                "quality_df = pd.DataFrame(quality_data)\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"QUALITY RESULTS\")\n",
                "print(\"=\" * 80)\n",
                "print(quality_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Comparison & Analysis <a name=\"comparison\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final summary comparison\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"FINAL COMPARISON SUMMARY\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "summary_data = []\n",
                "for method_name in ALL_WATERMARKERS.keys():\n",
                "    avg_attack_tpr = np.mean([attack_results[a][method_name][\"TPR@FPR=1%\"] for a in attack_results])\n",
                "    \n",
                "    summary_data.append({\n",
                "        \"Method\": method_name,\n",
                "        \"Detection AUC\": all_metrics[method_name][\"auc\"],\n",
                "        \"TPR@1% (Clean)\": all_metrics[method_name][\"tpr_at_fpr_1\"],\n",
                "        \"Avg TPR@1% (Attacked)\": avg_attack_tpr,\n",
                "        \"Perplexity\": ppl_results[method_name][\"mean\"],\n",
                "        \"Type\": \"GPW\" if \"GPW\" in method_name else \"Prior Work\",\n",
                "    })\n",
                "\n",
                "summary_df = pd.DataFrame(summary_data)\n",
                "print(summary_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Key findings\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"KEY FINDINGS\")\n",
                "print(\"=\" * 80)\n",
                "print(\"\"\"\n",
                "PRIOR WORK:\n",
                "- Unigram: Fixed green list, robust to random edits\n",
                "- KGW: Context-dependent, better security but lower robustness\n",
                "- SEMSTAMP: Sentence-level, robust to paraphrasing\n",
                "\n",
                "OUR GPW METHODS:\n",
                "- GPW: Basic cosine scoring with secret direction\n",
                "- GPW-SP: Context-keyed phase for better security\n",
                "- GPW-SP+SR: Hidden state coupling for semantic awareness\n",
                "\n",
                "TRADE-OFFS:\n",
                "- Higher alpha/omega = stronger watermark, potentially lower quality\n",
                "- Salted phase improves security against key guessing\n",
                "- SR coupling adds semantic context but increases computation\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save all results\n",
                "summary_df.to_csv(\"watermark_comparison_results.csv\", index=False)\n",
                "robustness_df.to_csv(\"attack_robustness_results.csv\", index=False)\n",
                "quality_df.to_csv(\"quality_results.csv\", index=False)\n",
                "\n",
                "print(\"Results saved to CSV files!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
