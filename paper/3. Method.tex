\section{Method}
\label{sec:method}

We describe our watermarking sampler as a sequence of increasingly structured designs.
We start from a naive ``Gaussian pancakes'' sampler, then add a salted phase for unpredictability.
Throughout, we focus on \textbf{private detection}: a verifier with a secret key can test a text, while an attacker who does not know the key should not be able to reliably predict or remove the watermark without substantially rewriting the text.

\subsection{Problem setup and notation}
We consider an autoregressive language model with vocabulary $\mathcal{V}$.
At generation step $t$, given a prefix $x_{1:t-1}$, the model outputs logits
\[
\ell_t \in \mathbb{R}^{|\mathcal{V}|},
\]
and a base distribution
\[
p_t(i) = \mathrm{softmax}(\ell_t/\tau)_i,
\]
with temperature $\tau>0$.
Let $E\in\mathbb{R}^{|\mathcal{V}|\times d}$ be the model's token embedding matrix, where token $i$ has embedding $e_i\in\mathbb{R}^d$.

A watermarking sampler produces a modified distribution $q_t$ that is close to $p_t$ (to preserve quality) but biased in a way that can be tested with the key.
We implement watermarking as an \emph{additive logit bias}:
\[
\ell'_t(i) = \ell_t(i) + b_t(i),
\qquad
q_t(i) \propto \exp(\ell'_t(i)/\tau).
\]
We choose $b_t(i)$ so that (i) the text remains natural, and (ii) a verifier can accumulate evidence across tokens.

\subsection{Stage 1: Gaussian Pancakes Watermarking (GPW)}
\label{sec:method:gpw}

\paragraph{Keyed secret direction.}
Given a secret key $K$, we derive a pseudorandom unit vector $w\in\mathbb{R}^d$:
\[
g \leftarrow \mathcal{N}(0,I_d)\ \text{seeded by }K,
\qquad
w = \frac{g}{\|g\|}.
\]
Intuition: $w$ defines a hidden axis in embedding space known only to the verifier.

\paragraph{Token projection.}
For each vocabulary token $i$, we compute and cache its projection on the secret axis:
\[
s_i = \langle e_i, w\rangle.
\]
This is computed once and reused for all generations.

\paragraph{Periodic ``pancake'' score.}
We define a periodic score on the projection coordinate:
\[
g(i) = \cos(\omega s_i),
\]
where $\omega>0$ controls frequency.
Large $\omega$ yields many thin alternating bands; smaller $\omega$ yields fewer thick bands.
We refer to these bands as ``pancakes'' because the cosine creates parallel high-score slices along $w$.

\paragraph{Logit bias and sampling.}
Given strength $\alpha \ge 0$, we bias logits toward high-score tokens:
\[
\ell'_t(i) = \ell_t(i) + \alpha \, g(i)
\quad\Rightarrow\quad
q_t(i) \propto p_t(i)\exp(\alpha g(i)).
\]
We then sample from $q_t$ using the same decoding settings as usual (temperature, top-$k$, top-$p$, etc.).

\paragraph{Why the naive design works (and where it fails).}
Under the base model distribution, the cosine score behaves like noise that averages to near zero over many tokens (especially when prompts vary).
Under the biased distribution, tokens with higher $g(i)$ become more likely, so the sum of scores tends to be positive.
However, the naive scheme has two weaknesses:
(1) the preference pattern is \emph{static} across positions, which can be exploited if an attacker can estimate the pattern from many samples; and
(2) because it is static, it can create small but consistent distortions that may be easier to wash out with paraphrasing.
These motivate the next stage.

\subsection{Stage 2: Salted-phase Gaussian Pancakes (GPW-SP)}
\label{sec:method:salt}

To prevent a static and predictable pattern, we make the cosine \emph{phase} depend on the local context through the secret key.

\paragraph{Salted phase from context.}
At each time step $t$, we compute a phase $\phi_t \in [0,2\pi)$:
\[
\phi_t = 2\pi \cdot \mathrm{Unif}\big(\mathrm{PRF}_K(\mathrm{ctx}_t)\big),
\]
where $\mathrm{PRF}_K(\cdot)$ is a keyed pseudorandom function and $\mathrm{ctx}_t$ is a short fingerprint of context.
In practice, $\mathrm{ctx}_t$ can be:
(i) the previous token $x_{t-1}$,
(ii) a hash of the last $n$ tokens,
or (iii) a rolling hash of the prefix (to reduce collisions).
The output of the PRF is converted to a uniform real in $[0,1)$ and then scaled to $[0,2\pi)$.

\paragraph{Salted pancake score.}
We now define a time-varying score:
\[
g_t(i) = \cos(\omega s_i + \phi_t).
\]
This makes the ``preferred'' bands move with context, so the watermark no longer corresponds to a fixed global partition.

\paragraph{Watermarked sampling distribution.}
We bias logits using the salted score:
\[
\ell'_t(i) = \ell_t(i) + \alpha \, g_t(i),
\qquad
q_t(i) \propto p_t(i)\exp(\alpha g_t(i)).
\]
Implementation is still simple: cache $s_i$, compute $\phi_t$ each step, evaluate $g_t(i)$, and add $\alpha g_t(i)$ to logits.

\paragraph{Optional efficiency restriction (top-$k$ / nucleus).}
To reduce computation and limit distortion, we can apply the bias only to a candidate set $\mathcal{C}_t$ (e.g., top-$k$ tokens under $\ell_t$ or the nucleus set under $p_t$):
\[
\ell'_t(i) = \ell_t(i) + \alpha \, g_t(i)\cdot \mathbb{I}[i\in \mathcal{C}_t].
\]
This preserves the same structure while improving speed and keeping the watermark inside the model's likely choices.

\paragraph{Detection statistic.}
Given a candidate text $x_{1:T}$ and key $K$, the verifier:
\begin{enumerate}
    \item Reconstructs $w$ from $K$.
    \item For each token $x_t$, computes $\phi_t$ from $\mathrm{ctx}_t$ and $K$.
    \item Computes per-token alignment
    \[
    a_t = \cos(\omega s_{x_t} + \phi_t).
    \]
    \item Aggregates
    \[
    S(x_{1:T}) = \sum_{t=1}^T a_t.
    \]
\end{enumerate}

\paragraph{Hypothesis test and calibration.}
We test
\[
H_0:\text{text is unwatermarked}
\quad\text{vs.}\quad
H_1:\text{text is generated by GPW-SP under key }K.
\]
Under $H_0$, $S$ is approximately centered near $0$ and concentrates as $T$ grows.
Under $H_1$, the bias makes $\mathbb{E}[a_t] > 0$, so $S$ tends to be larger.
In practice we normalize
\[
Z(x_{1:T}) = \frac{S(x_{1:T}) - \mu_0(T)}{\sigma_0(T)},
\]
where $\mu_0(T),\sigma_0(T)$ are estimated from a held-out corpus of human or non-watermarked text under the same tokenizer and preprocessing.
We then choose a threshold $z_\star$ such that $\Pr_{H_0}(Z\ge z_\star)$ matches a target false positive rate.
As an alternative view, we also report \emph{random-key $p$-values}: we evaluate $S$ under many random keys and compute the fraction that produce a score at least as large as the observed one.

\subsection{Payload encoding (optional extension)}
\label{sec:method:payload}

GPW-SP can be used for either \emph{presence detection} (one-bit: watermarked or not) or for embedding a short payload.

\paragraph{Segment-wise phase shifts.}
Let a payload be $m\in\{0,1\}^k$.
We encode it (optionally) with an error-correcting code (ECC) to obtain a codeword $c\in\{0,1\}^L$.
We then divide generation into segments of length $R$ tokens.
For segment index $j$, we set
\[
\Delta_j = \pi \, c_j,
\]
and use
\[
g_t(i) = \cos(\omega s_i + \phi_t + \Delta_j),
\qquad \text{for } t \in \text{segment } j.
\]
This makes the watermark statistic favor one of two phase hypotheses per segment.
The verifier tests both hypotheses and decodes the resulting bit sequence.

\paragraph{Decoding.}
Given a text, the verifier computes for each segment $j$ two correlation scores (for $\Delta_j=0$ and $\Delta_j=\pi$), selects the higher-scoring hypothesis as the recovered bit $\hat{c}_j$, then ECC-decodes $\hat{c}$ to obtain $\hat{m}$.
We can also add repetition (use multiple segments per bit) to increase robustness to deletions and truncation.

\subsection{Summary of parameters}
Our sampler uses a small set of interpretable parameters:
\begin{itemize}
    \item $\alpha$ (strength): larger values increase detectability but may affect quality.
    \item $\omega$ (frequency): controls pancake ``thinness'' and the granularity of the periodic pattern.
    \item Choice of $\mathrm{ctx}_t$ for salted phase: trades off unpredictability vs.\ stability.
    \item Optional restriction set $\mathcal{C}_t$ (top-$k$/top-$p$): reduces compute and limits distortion.
\end{itemize}

\subsection{Stage 3: Semantic Representation Coupling (GPW-SP-SR)}
\label{sec:method:sr}

While GPW-SP provides unpredictability through context-dependent phase shifts, it relies solely on the static token embedding projections $s_i = \langle e_i, w \rangle$. When an attacker paraphrases text, they may replace tokens with semantically similar alternatives that happen to have very different projection values, degrading the watermark signal. To address this, we introduce \textbf{semantic representation coupling} (SR), which incorporates the model's contextual hidden states into the watermark computation.

\paragraph{Contextual embedding extraction.}
At generation step $t$, after computing the model's forward pass, we extract the hidden state $h_t \in \mathbb{R}^{d_h}$ from the final transformer layer (before the language model head). This hidden state captures the contextual meaning of the position, not just the static token identity.

\paragraph{Semantic projection.}
We derive a second secret direction $w_{\text{sr}} \in \mathbb{R}^{d_h}$ from the key $K$ (using a different seed or hash):
\[
g_{\text{sr}} \leftarrow \mathcal{N}(0, I_{d_h})\ \text{seeded by } \mathrm{PRF}_K(\texttt{"sr"}),
\qquad
w_{\text{sr}} = \frac{g_{\text{sr}}}{\|g_{\text{sr}}\|}.
\]
We then compute a contextual score based on the hidden state:
\[
r_t = \langle h_t, w_{\text{sr}} \rangle.
\]

\paragraph{Combined score with SR coupling.}
The GPW-SP-SR score combines the static token projection with the contextual representation:
\[
g_t^{\text{SR}}(i) = \cos\big(\omega s_i + \phi_t + \beta \cdot r_t\big),
\]
where $\beta > 0$ is a coupling strength parameter that controls how much the contextual representation influences the phase. When $\beta = 0$, this reduces to standard GPW-SP.

\paragraph{Intuition: semantic stability.}
The key insight is that semantically similar tokens in similar contexts tend to produce similar hidden states $h_t$, even if their static embeddings $e_i$ differ substantially. By coupling the watermark phase to $r_t$, we create a signal that is more stable under meaning-preserving edits:
\begin{itemize}[noitemsep]
    \item If an attacker replaces ``happy'' with ``joyful'', the contextual hidden state remains similar, so $r_t$ changes little.
    \item The phase shift $\beta \cdot r_t$ thus provides continuity across paraphrases.
    \item Meanwhile, the base projection $\omega s_i + \phi_t$ still provides the core watermark signal.
\end{itemize}

\paragraph{Detection with SR coupling.}
During detection, the verifier must re-run the language model on the candidate text to obtain hidden states $\{h_t\}$. The detection statistic becomes:
\[
S^{\text{SR}}(x_{1:T}) = \sum_{t=1}^T \cos\big(\omega s_{x_t} + \phi_t + \beta \cdot r_t\big),
\]
where $r_t = \langle h_t, w_{\text{sr}} \rangle$ is computed from the model's hidden state at position $t$.

\paragraph{Trade-offs.}
GPW-SP-SR provides improved robustness to paraphrasing at the cost of:
\begin{enumerate}[noitemsep]
    \item \textbf{Detection overhead}: Verification requires a full model forward pass, not just tokenization.
    \item \textbf{Model dependency}: The detector must have access to the same (or compatible) model used for generation.
    \item \textbf{Additional hyperparameter}: The coupling strength $\beta$ must be tuned.
\end{enumerate}
In practice, we find $\beta \in [0.1, 1.0]$ works well, with the optimal value depending on the model and attack distribution.

\subsection{What we implement and evaluate}
In experiments, we evaluate:
\begin{enumerate}[noitemsep]
    \item[(i)] \textbf{GPW} (static phase): Simple baseline with fixed pancake bands.
    \item[(ii)] \textbf{GPW-SP} (salted phase): Context-dependent phase for unpredictability.
    \item[(iii)] \textbf{GPW-SP-SR} (salted phase + semantic representation): Full method with contextual coupling for paraphrase robustness.
\end{enumerate}