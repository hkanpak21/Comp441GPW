{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Paraphrase Attack Experiment\n",
    "\n",
    "This notebook runs **complete** paraphrase attack experiments:\n",
    "1. Generates watermarked texts for all model/watermarker combinations\n",
    "2. Applies paraphrase attack using Pegasus\n",
    "3. Runs watermark detection\n",
    "4. Outputs comprehensive CSV results\n",
    "\n",
    "**Models:** OPT-1.3B, GPT-2, Qwen-7B  \n",
    "**Watermarkers:** GPW, GPW-SP, GPW-SP-LOW, Unigram, KGW  \n",
    "**Samples:** 50 per combination\n",
    "\n",
    "**Output:** `paraphrase_complete_results.csv` with all per-sample results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Install Dependencies\n",
    "!pip install -q torch transformers sentencepiece tqdm pandas scipy nltk accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Download NLTK Data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "print(\"NLTK data downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 3. Configuration\nimport torch\n\n# Experiment settings\nN_SAMPLES = 50\nMAX_NEW_TOKENS = 200\nZ_THRESHOLD = 4.0\n\n# Models to test\nMODELS = [\n    (\"opt-1.3b\", \"facebook/opt-1.3b\"),\n    (\"gpt2\", \"gpt2\"),\n    # (\"qwen-7b\", \"Qwen/Qwen2-7B\"),  # Uncomment if you have enough GPU memory\n]\n\n# Watermarkers to test (now includes gpw_sp_sr)\nWATERMARKERS = [\"gpw\", \"gpw_sp\", \"gpw_sp_low\", \"gpw_sp_sr\", \"unigram\", \"kgw\"]\n\n# Check GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nif device == \"cuda\":\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\nprint(f\"\\nExperiment Configuration:\")\nprint(f\"  Samples per experiment: {N_SAMPLES}\")\nprint(f\"  Models: {[m[0] for m in MODELS]}\")\nprint(f\"  Watermarkers: {WATERMARKERS}\")\nprint(f\"  Total experiments: {len(MODELS) * len(WATERMARKERS)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Prompts for Text Generation\n",
    "PROMPTS = [\n",
    "    \"The future of artificial intelligence will\",\n",
    "    \"Climate change is affecting our planet because\",\n",
    "    \"The history of computing began when\",\n",
    "    \"In modern society, technology has\",\n",
    "    \"Scientists have discovered that\",\n",
    "    \"The importance of education lies in\",\n",
    "    \"Economic growth depends on\",\n",
    "    \"The human brain is remarkable because\",\n",
    "    \"Space exploration has revealed\",\n",
    "    \"The art of storytelling involves\",\n",
    "    \"Medical advances have helped\",\n",
    "    \"The ocean covers most of Earth and\",\n",
    "    \"Ancient civilizations developed\",\n",
    "    \"Music affects human emotions by\",\n",
    "    \"The principles of democracy include\",\n",
    "    \"Renewable energy sources are\",\n",
    "    \"The study of genetics reveals\",\n",
    "    \"Urban planning must consider\",\n",
    "    \"The psychology of decision making\",\n",
    "    \"Global trade has transformed\",\n",
    "    \"The evolution of language shows\",\n",
    "    \"Environmental conservation requires\",\n",
    "    \"The philosophy of ethics addresses\",\n",
    "    \"Digital communication has changed\",\n",
    "    \"The architecture of ancient Rome\",\n",
    "    \"Biodiversity is essential because\",\n",
    "    \"The mathematics of probability\",\n",
    "    \"Social media platforms have\",\n",
    "    \"The chemistry of cooking involves\",\n",
    "    \"Historical events shape our\",\n",
    "    \"The physics of light explains\",\n",
    "    \"Cultural traditions preserve\",\n",
    "    \"The economics of healthcare\",\n",
    "    \"Artificial neural networks can\",\n",
    "    \"The geography of continents\",\n",
    "    \"Public health initiatives aim to\",\n",
    "    \"The literature of the 19th century\",\n",
    "    \"Quantum mechanics describes\",\n",
    "    \"The sociology of communities\",\n",
    "    \"Agricultural innovation has\",\n",
    "    \"The engineering behind bridges\",\n",
    "    \"Political systems vary because\",\n",
    "    \"The astronomy of distant galaxies\",\n",
    "    \"Mental health awareness helps\",\n",
    "    \"The anthropology of human cultures\",\n",
    "    \"Sustainable development requires\",\n",
    "    \"The linguistics of grammar shows\",\n",
    "    \"International relations depend on\",\n",
    "    \"The meteorology of storms\",\n",
    "    \"Educational technology enables\",\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(PROMPTS)} prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 5. Watermarker Implementations\nimport hashlib\nimport numpy as np\nfrom scipy import stats\nimport torch.nn.functional as F\n\nclass BaseWatermarker:\n    \"\"\"Base class for watermarkers.\"\"\"\n    def __init__(self, model, tokenizer, device):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        # Get vocab size from model's output dimension\n        if hasattr(model, 'lm_head'):\n            self.vocab_size = model.lm_head.out_features\n        elif hasattr(model, 'get_output_embeddings'):\n            out_emb = model.get_output_embeddings()\n            if out_emb is not None:\n                self.vocab_size = out_emb.weight.shape[0]\n            else:\n                self.vocab_size = tokenizer.vocab_size\n        else:\n            self.vocab_size = tokenizer.vocab_size\n        print(f\"    Using vocab_size: {self.vocab_size}\")\n\n    def generate(self, prompt, max_new_tokens=200):\n        raise NotImplementedError\n\n    def detect(self, text):\n        raise NotImplementedError\n\n\nclass GPWWatermarker(BaseWatermarker):\n    \"\"\"GPW watermarker with green/red list based on previous token.\"\"\"\n    def __init__(self, model, tokenizer, device, omega=2.0, z_threshold=4.0):\n        super().__init__(model, tokenizer, device)\n        self.omega = omega\n        self.z_threshold = z_threshold\n\n    def _get_green_indices(self, prev_token, size):\n        \"\"\"Get green list indices based on previous token.\"\"\"\n        hash_input = str(prev_token).encode()\n        hash_val = int(hashlib.sha256(hash_input).hexdigest(), 16)\n        rng = np.random.RandomState(hash_val % (2**31))\n        green_size = int(size * 0.5)\n        indices = rng.permutation(size)\n        return indices[:green_size]\n\n    def generate(self, prompt, max_new_tokens=200):\n        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n        generated = input_ids.clone()\n\n        for _ in range(max_new_tokens):\n            with torch.no_grad():\n                outputs = self.model(generated)\n                logits = outputs.logits[:, -1, :].clone()  # Clone to avoid in-place issues\n                vocab_size = logits.shape[-1]\n\n            # Get green indices and apply bias\n            prev_token = generated[0, -1].item()\n            green_indices = self._get_green_indices(prev_token, vocab_size)\n            \n            # Apply bias using index_add for safety\n            bias = torch.zeros_like(logits)\n            bias[0, green_indices] = self.omega\n            logits = logits + bias\n\n            # Sample with numerical stability\n            logits = logits.float()\n            probs = F.softmax(logits / 0.8, dim=-1)\n            probs = torch.clamp(probs, min=1e-9)\n            probs = probs / probs.sum(dim=-1, keepdim=True)\n            \n            next_token = torch.multinomial(probs, 1)\n            generated = torch.cat([generated, next_token], dim=-1)\n\n            if next_token.item() == self.tokenizer.eos_token_id:\n                break\n\n        return self.tokenizer.decode(generated[0], skip_special_tokens=True)\n\n    def detect(self, text):\n        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n        if len(tokens) < 2:\n            return {'z_score': 0, 'p_value': 1.0, 'is_detected': False}\n\n        green_count = 0\n        total = 0\n\n        for i in range(1, len(tokens)):\n            prev_token = tokens[i-1]\n            curr_token = tokens[i]\n            green_indices = self._get_green_indices(prev_token, self.vocab_size)\n            green_set = set(green_indices)\n\n            if curr_token < self.vocab_size and curr_token in green_set:\n                green_count += 1\n            total += 1\n\n        if total == 0:\n            return {'z_score': 0, 'p_value': 1.0, 'is_detected': False}\n\n        p = 0.5\n        observed = green_count / total\n        z_score = (observed - p) / np.sqrt(p * (1-p) / total)\n        p_value = 1 - stats.norm.cdf(z_score)\n\n        return {'z_score': z_score, 'p_value': p_value, 'is_detected': z_score >= self.z_threshold}\n\n\nclass GPWSPWatermarker(GPWWatermarker):\n    \"\"\"GPW-SP: Salted phase variant.\"\"\"\n    def __init__(self, model, tokenizer, device, omega=2.0, z_threshold=4.0):\n        super().__init__(model, tokenizer, device, omega, z_threshold)\n        self.salt = np.random.randint(0, 2**31)\n        print(f\"    GPW-SP salt: {self.salt}\")\n\n    def _get_green_indices(self, prev_token, size):\n        \"\"\"Get green list indices based on previous token and salt.\"\"\"\n        hash_input = f\"{prev_token}_{self.salt}\".encode()\n        hash_val = int(hashlib.sha256(hash_input).hexdigest(), 16)\n        rng = np.random.RandomState(hash_val % (2**31))\n        green_size = int(size * 0.5)\n        indices = rng.permutation(size)\n        return indices[:green_size]\n\n\nclass GPWSPLowWatermarker(GPWSPWatermarker):\n    \"\"\"GPW-SP-LOW: Salted phase with lower omega.\"\"\"\n    def __init__(self, model, tokenizer, device, omega=1.0, z_threshold=4.0):\n        super().__init__(model, tokenizer, device, omega, z_threshold)\n\n\nclass GPWSPSRWatermarker(GPWSPWatermarker):\n    \"\"\"GPW-SP-SR: Salted phase with Semantic Relatedness detection.\n    \n    Note: This uses a simplified SR detection based on bigram consistency.\n    The full SR implementation would require SentenceTransformer.\n    \"\"\"\n    def __init__(self, model, tokenizer, device, omega=2.0, z_threshold=4.0):\n        super().__init__(model, tokenizer, device, omega, z_threshold)\n        print(f\"    GPW-SP-SR (simplified SR detection)\")\n\n    def detect(self, text):\n        \"\"\"Simplified SR detection - uses windowed approach.\"\"\"\n        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n        if len(tokens) < 10:\n            return {'z_score': 0, 'p_value': 1.0, 'is_detected': False}\n\n        # Use sliding windows for more robust detection\n        window_size = 25\n        window_scores = []\n        \n        for start in range(0, len(tokens) - window_size + 1, 10):\n            window_tokens = tokens[start:start + window_size]\n            green_count = 0\n            total = 0\n            \n            for i in range(1, len(window_tokens)):\n                prev_token = window_tokens[i-1]\n                curr_token = window_tokens[i]\n                green_indices = self._get_green_indices(prev_token, self.vocab_size)\n                green_set = set(green_indices)\n                \n                if curr_token < self.vocab_size and curr_token in green_set:\n                    green_count += 1\n                total += 1\n            \n            if total > 0:\n                window_scores.append(green_count / total)\n        \n        if not window_scores:\n            return {'z_score': 0, 'p_value': 1.0, 'is_detected': False}\n        \n        # Use median of window scores for robustness\n        observed = np.median(window_scores)\n        n = len(tokens) - 1\n        p = 0.5\n        z_score = (observed - p) / np.sqrt(p * (1-p) / n)\n        p_value = 1 - stats.norm.cdf(z_score)\n\n        return {'z_score': z_score, 'p_value': p_value, 'is_detected': z_score >= self.z_threshold}\n\n\nclass UnigramWatermarker(BaseWatermarker):\n    \"\"\"Unigram watermarker with fixed green list.\"\"\"\n    def __init__(self, model, tokenizer, device, delta=2.0, z_threshold=4.0):\n        super().__init__(model, tokenizer, device)\n        self.delta = delta\n        self.z_threshold = z_threshold\n        # Pre-compute green list with fixed seed\n        rng = np.random.RandomState(42)\n        self.green_indices = rng.permutation(self.vocab_size)[:int(self.vocab_size * 0.5)]\n        self.green_set = set(self.green_indices)\n\n    def generate(self, prompt, max_new_tokens=200):\n        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n        generated = input_ids.clone()\n\n        for _ in range(max_new_tokens):\n            with torch.no_grad():\n                outputs = self.model(generated)\n                logits = outputs.logits[:, -1, :].clone()\n                vocab_size = logits.shape[-1]\n\n            # Create bias - handle vocab size mismatch\n            bias = torch.zeros_like(logits)\n            valid_indices = self.green_indices[self.green_indices < vocab_size]\n            bias[0, valid_indices] = self.delta\n            logits = logits + bias\n\n            logits = logits.float()\n            probs = F.softmax(logits / 0.8, dim=-1)\n            probs = torch.clamp(probs, min=1e-9)\n            probs = probs / probs.sum(dim=-1, keepdim=True)\n            \n            next_token = torch.multinomial(probs, 1)\n            generated = torch.cat([generated, next_token], dim=-1)\n\n            if next_token.item() == self.tokenizer.eos_token_id:\n                break\n\n        return self.tokenizer.decode(generated[0], skip_special_tokens=True)\n\n    def detect(self, text):\n        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n        if len(tokens) < 1:\n            return {'z_score': 0, 'p_value': 1.0, 'is_detected': False}\n\n        green_count = sum(1 for t in tokens if t in self.green_set)\n        total = len(tokens)\n\n        p = 0.5\n        observed = green_count / total\n        z_score = (observed - p) / np.sqrt(p * (1-p) / total)\n        p_value = 1 - stats.norm.cdf(z_score)\n\n        return {'z_score': z_score, 'p_value': p_value, 'is_detected': z_score >= self.z_threshold}\n\n\nclass KGWWatermarker(BaseWatermarker):\n    \"\"\"KGW watermarker with context-based green list.\"\"\"\n    def __init__(self, model, tokenizer, device, gamma=0.5, delta=2.0, z_threshold=4.0):\n        super().__init__(model, tokenizer, device)\n        self.gamma = gamma\n        self.delta = delta\n        self.z_threshold = z_threshold\n\n    def _get_green_indices(self, context, size):\n        \"\"\"Get green list indices based on context.\"\"\"\n        context_hash = hash(tuple(context)) % (2**31)\n        rng = np.random.RandomState(context_hash)\n        green_size = int(size * self.gamma)\n        indices = rng.permutation(size)\n        return indices[:green_size]\n\n    def generate(self, prompt, max_new_tokens=200):\n        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n        generated = input_ids.clone()\n\n        for _ in range(max_new_tokens):\n            with torch.no_grad():\n                outputs = self.model(generated)\n                logits = outputs.logits[:, -1, :].clone()\n                vocab_size = logits.shape[-1]\n\n            context = generated[0, -4:].tolist()\n            green_indices = self._get_green_indices(context, vocab_size)\n            \n            bias = torch.zeros_like(logits)\n            bias[0, green_indices] = self.delta\n            logits = logits + bias\n\n            logits = logits.float()\n            probs = F.softmax(logits / 0.8, dim=-1)\n            probs = torch.clamp(probs, min=1e-9)\n            probs = probs / probs.sum(dim=-1, keepdim=True)\n            \n            next_token = torch.multinomial(probs, 1)\n            generated = torch.cat([generated, next_token], dim=-1)\n\n            if next_token.item() == self.tokenizer.eos_token_id:\n                break\n\n        return self.tokenizer.decode(generated[0], skip_special_tokens=True)\n\n    def detect(self, text):\n        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n        if len(tokens) < 2:\n            return {'z_score': 0, 'p_value': 1.0, 'is_detected': False}\n\n        green_count = 0\n        total = 0\n\n        for i in range(1, len(tokens)):\n            context = tokens[max(0, i-4):i]\n            green_indices = self._get_green_indices(context, self.vocab_size)\n            green_set = set(green_indices)\n\n            if tokens[i] < self.vocab_size and tokens[i] in green_set:\n                green_count += 1\n            total += 1\n\n        if total == 0:\n            return {'z_score': 0, 'p_value': 1.0, 'is_detected': False}\n\n        p = self.gamma\n        observed = green_count / total\n        z_score = (observed - p) / np.sqrt(p * (1-p) / total)\n        p_value = 1 - stats.norm.cdf(z_score)\n\n        return {'z_score': z_score, 'p_value': p_value, 'is_detected': z_score >= self.z_threshold}\n\n\ndef get_watermarker(name, model, tokenizer, device):\n    \"\"\"Factory function to create watermarker.\"\"\"\n    watermarkers = {\n        \"gpw\": GPWWatermarker,\n        \"gpw_sp\": GPWSPWatermarker,\n        \"gpw_sp_low\": GPWSPLowWatermarker,\n        \"gpw_sp_sr\": GPWSPSRWatermarker,\n        \"unigram\": UnigramWatermarker,\n        \"kgw\": KGWWatermarker,\n    }\n    if name not in watermarkers:\n        raise ValueError(f\"Unknown watermarker: {name}. Available: {list(watermarkers.keys())}\")\n    return watermarkers[name](model, tokenizer, device)\n\nprint(\"Watermarker classes defined!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. Load Pegasus Paraphraser\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import re\n",
    "\n",
    "print(\"Loading Pegasus model for paraphrasing...\")\n",
    "pegasus_model_name = \"tuner007/pegasus_paraphrase\"\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_name)\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_model_name).to(device)\n",
    "pegasus_model.eval()\n",
    "print(\"Pegasus loaded!\")\n",
    "\n",
    "def paraphrase_text(text):\n",
    "    \"\"\"Paraphrase a text by paraphrasing each sentence.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    paraphrased_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if not sentence.strip():\n",
    "            continue\n",
    "\n",
    "        if len(sentence) > 400:\n",
    "            sentence = sentence[:400]\n",
    "\n",
    "        sentence = sentence.encode('ascii', 'ignore').decode('ascii').strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            inputs = pegasus_tokenizer(\n",
    "                sentence,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=100,\n",
    "                truncation=True\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = pegasus_model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=100,\n",
    "                    num_beams=4,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "\n",
    "            paraphrased = pegasus_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            paraphrased_sentences.append(paraphrased)\n",
    "        except:\n",
    "            paraphrased_sentences.append(sentence)\n",
    "\n",
    "    return ' '.join(paraphrased_sentences)\n",
    "\n",
    "print(\"Paraphrase function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7. Run Complete Experiments\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for model_name, model_path in MODELS:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"MODEL: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Load model\n",
    "    print(f\"Loading {model_path}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "    print(f\"Model loaded!\")\n",
    "\n",
    "    for wm_name in WATERMARKERS:\n",
    "        print(f\"\\n  --- Watermarker: {wm_name} ---\")\n",
    "\n",
    "        # Create watermarker\n",
    "        watermarker = get_watermarker(wm_name, model, tokenizer, device)\n",
    "\n",
    "        # Generate watermarked texts\n",
    "        print(f\"  Generating {N_SAMPLES} watermarked texts...\")\n",
    "        generated_texts = []\n",
    "        for i in tqdm(range(N_SAMPLES), desc=\"  Generating\", leave=False):\n",
    "            prompt = PROMPTS[i % len(PROMPTS)]\n",
    "            try:\n",
    "                text = watermarker.generate(prompt, max_new_tokens=MAX_NEW_TOKENS)\n",
    "                generated_texts.append(text)\n",
    "            except Exception as e:\n",
    "                print(f\"    Error generating sample {i}: {e}\")\n",
    "                generated_texts.append(prompt)\n",
    "\n",
    "        # Paraphrase texts\n",
    "        print(f\"  Paraphrasing {len(generated_texts)} texts...\")\n",
    "        paraphrased_texts = []\n",
    "        for text in tqdm(generated_texts, desc=\"  Paraphrasing\", leave=False):\n",
    "            try:\n",
    "                paraphrased = paraphrase_text(text)\n",
    "                paraphrased_texts.append(paraphrased)\n",
    "            except Exception as e:\n",
    "                paraphrased_texts.append(text)\n",
    "\n",
    "        # Detect watermarks\n",
    "        print(f\"  Running detection...\")\n",
    "        detected_count = 0\n",
    "        for i, text in enumerate(tqdm(paraphrased_texts, desc=\"  Detecting\", leave=False)):\n",
    "            try:\n",
    "                result = watermarker.detect(text)\n",
    "                z_score = result['z_score']\n",
    "                is_detected = result['is_detected']\n",
    "\n",
    "                all_results.append({\n",
    "                    'model': model_name,\n",
    "                    'watermarker': wm_name,\n",
    "                    'attack': 'paraphrase',\n",
    "                    'sample_idx': i,\n",
    "                    'z_score': z_score,\n",
    "                    'p_value': result['p_value'],\n",
    "                    'is_detected': 1 if is_detected else 0\n",
    "                })\n",
    "\n",
    "                if is_detected:\n",
    "                    detected_count += 1\n",
    "            except Exception as e:\n",
    "                all_results.append({\n",
    "                    'model': model_name,\n",
    "                    'watermarker': wm_name,\n",
    "                    'attack': 'paraphrase',\n",
    "                    'sample_idx': i,\n",
    "                    'z_score': 0,\n",
    "                    'p_value': 1.0,\n",
    "                    'is_detected': 0\n",
    "                })\n",
    "\n",
    "        detection_rate = detected_count / len(paraphrased_texts) * 100\n",
    "        print(f\"  Result: {detection_rate:.1f}% ({detected_count}/{len(paraphrased_texts)})\")\n",
    "\n",
    "    # Free model memory\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ALL EXPERIMENTS COMPLETE!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8. Create Results DataFrame and Summary\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Calculate summary statistics\n",
    "summary = df.groupby(['model', 'watermarker']).agg({\n",
    "    'is_detected': ['sum', 'count', 'mean'],\n",
    "    'z_score': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "summary.columns = ['detected_count', 'total_samples', 'detection_rate', 'mean_z_score']\n",
    "summary['detection_rate'] = (summary['detection_rate'] * 100).round(2)\n",
    "summary = summary.reset_index()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY OF RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# Display per-model breakdown\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED VIEW\")\n",
    "print(\"=\"*70)\n",
    "for model in df['model'].unique():\n",
    "    print(f\"\\n{model}:\")\n",
    "    model_df = summary[summary['model'] == model]\n",
    "    for _, row in model_df.iterrows():\n",
    "        print(f\"  {row['watermarker']:15} - Detection: {row['detection_rate']:5.1f}% | Mean Z: {row['mean_z_score']:6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9. Save Results to CSV\n",
    "from google.colab import files\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save per-sample results\n",
    "csv_filename = f\"paraphrase_complete_results_{timestamp}.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Per-sample results saved to: {csv_filename}\")\n",
    "\n",
    "# Save summary\n",
    "summary_csv = f\"paraphrase_summary_{timestamp}.csv\"\n",
    "summary.to_csv(summary_csv, index=False)\n",
    "print(f\"Summary saved to: {summary_csv}\")\n",
    "\n",
    "# Save as JSON for easy parsing\n",
    "summary_json = f\"paraphrase_summary_{timestamp}.json\"\n",
    "summary_dict = summary.to_dict(orient='records')\n",
    "with open(summary_json, 'w') as f:\n",
    "    json.dump(summary_dict, f, indent=2)\n",
    "print(f\"Summary JSON saved to: {summary_json}\")\n",
    "\n",
    "# Preview\n",
    "print(\"\\nFirst 10 rows of results:\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10. Download All Results\n",
    "print(\"Downloading per-sample results CSV...\")\n",
    "files.download(csv_filename)\n",
    "\n",
    "print(\"Downloading summary CSV...\")\n",
    "files.download(summary_csv)\n",
    "\n",
    "print(\"Downloading summary JSON...\")\n",
    "files.download(summary_json)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nYou can now update unified_results.csv with the paraphrase attack data.\")\n",
    "print(\"\\nFormat for unified_results.csv:\")\n",
    "print(\"Model,Watermarker,Variant,Alpha,Omega,Z_Threshold,Attack,Detection_Rate,Detection_Count,Total_Samples,Mean_Z_Score,Perplexity,Notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### To add Qwen-7B:\n",
    "Uncomment the Qwen line in the MODELS configuration. Note that Qwen-7B requires more GPU memory.\n",
    "\n",
    "### Updating unified_results.csv:\n",
    "For each row in the summary, add to unified_results.csv:\n",
    "```\n",
    "OPT-1.3B,GPW,GPW (non-salted),3.0,50.0,4.0,paraphrase,XX.X,XX,50,X.XX,-,Paraphrase attack\n",
    "```\n",
    "\n",
    "### Troubleshooting:\n",
    "- If you run out of GPU memory, reduce N_SAMPLES or run one model at a time\n",
    "- If Pegasus fails on some texts, they will use the original text (counted as failed paraphrase)"
   ]
  }
 ]
}