# -*- coding: utf-8 -*-
"""GPW_SP_Contextual_Cluster_WM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NOAc1BUHeqN_llgoAbVT4LB7zDV0mNcL

# GPW Watermarking Notebook

This notebook implements our watermarking sampler in three stages:
1) GPW: Gaussian Pancakes Watermarking (static cosine score)
2) GPW-SP: GPW with Salted Phase (context-keyed phase)
3) GPW-SP+SR: Salted phase + Semantic Representation Coupling (hidden-state dependent direction)

Then we provide a starter experimental suite:
- clean detectability (TPR@FPR, ROC/AUC)
- robustness hooks (substitution/paraphrase/translation/truncation)
- ablations (methodology + hyperparameter sweeps)
- comparison hooks (MarkLLM + WaterPark)

References (used to motivate evaluation + baselines):
- Kirchenbauer et al. 2023 (green-list watermark)
- Pan et al. 2024 (MarkLLM toolkit)
- Liang et al. 2025 (WaterPark benchmark / Watermark under Fire)
- Tu et al. 2024 (WaterBench)
- Kuditipudi et al. 2024 (RDF / distortion-free)
- Pan et al. 2025 (distillation inheritance)
"""

!pip -q install transformers accelerate sentencepiece datasets evaluate scikit-learn

import os, math, hashlib, random, time
import json, glob, math, random
from dataclasses import dataclass
from typing import Optional, Dict, Any, List, Tuple
import pandas as pd
from tqdm import tqdm
import numpy as np
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM

from sklearn.metrics import roc_auc_score, roc_curve

# Repro
GLOBAL_SEED = 1234
random.seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)
torch.manual_seed(GLOBAL_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(GLOBAL_SEED)

device = "cuda" if torch.cuda.is_available() else "cpu"
device

MODEL_NAME = "gpt2"   # swap later
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)
model.eval()

# GPT-2 has no pad token by default
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

"""## Crypto Utils"""

def _hmac_like(key: bytes, msg: bytes) -> bytes:
    # Lightweight "PRF-like" keyed hash (not a formal HMAC implementation).
    # For a paper prototype this is typically fine; for production use HMAC-SHA256.
    return hashlib.sha256(key + b"||" + msg).digest()

def prf_to_uniform01(key: bytes, msg: bytes) -> float:
    # Map PRF output to [0,1)
    digest = _hmac_like(key, msg)
    x = int.from_bytes(digest[:8], "big")  # 64-bit
    return (x % (2**53)) / float(2**53)

def seeded_torch_generator(key: bytes, tag: bytes, device: str) -> torch.Generator:
    # Make a torch.Generator whose device matches the tensors you'll sample.
    seed_bytes = _hmac_like(key, tag)
    seed = int.from_bytes(seed_bytes[:8], "big") % (2**63 - 1)
    g = torch.Generator(device=device)
    g.manual_seed(seed)
    return g

@dataclass
class DecodeConfig:
    temperature: float = 1.0
    top_k: int = 0         # 0 disables top-k
    top_p: float = 1.0     # 1.0 disables nucleus
    max_new_tokens: int = 128

@dataclass
class GPWConfig:
    alpha: float = 1.0
    omega: float = 10.0
    salted: bool = True
    ctx_mode: str = "prev_token"   # "prev_token" | "ngram" | "rolling"
    ngram: int = 4                 # used if ctx_mode == "ngram"

@dataclass
class SRConfig:
    enabled: bool = False
    lambda_couple: float = 0.2
    rank: int = 8

@dataclass
class PayloadConfig:
    enabled: bool = False
    segment_len: int = 32
    # Placeholder: add ECC params later (e.g., repetition, Hamming, BCH)

@torch.no_grad()
def get_token_embedding_matrix(model) -> torch.Tensor:
    # Works for most HF causal LMs; for some models you may need model.get_input_embeddings().weight
    return model.get_input_embeddings().weight  # [V, d]

@torch.no_grad()
def derive_secret_direction_w(key: bytes, d: int, device: str) -> torch.Tensor:
    g = seeded_torch_generator(key, b"GPW_w", device=device)
    v = torch.randn(d, generator=g, device=device, dtype=torch.float32)
    v = v / (v.norm() + 1e-12)
    return v

@torch.no_grad()
def precompute_projections(E: torch.Tensor, w: torch.Tensor) -> torch.Tensor:
    # E: [V,d], w: [d] => s: [V]
    return (E @ w).float()

def ctx_fingerprint(input_ids: torch.Tensor, mode: str, ngram: int = 4) -> bytes:
    """
    input_ids: [1, t] on device
    returns: bytes fingerprint for PRF input
    """
    ids = input_ids[0].tolist()
    if len(ids) == 0:
        return b"empty"

    if mode == "prev_token":
        return f"prev:{ids[-1]}".encode()

    if mode == "ngram":
        tail = ids[-ngram:] if len(ids) >= ngram else ids
        return ("ngram:" + ",".join(map(str, tail))).encode()

    if mode == "rolling":
        # rolling hash of the prefix (cheap, stable)
        h = hashlib.sha256(("roll:" + ",".join(map(str, ids[-64:]))).encode()).digest()
        return b"roll:" + h[:16]

    raise ValueError(f"Unknown ctx_mode={mode}")

def salted_phase_phi(key: bytes, input_ids: torch.Tensor, cfg: GPWConfig) -> float:
    fp = ctx_fingerprint(input_ids, cfg.ctx_mode, cfg.ngram)
    u = prf_to_uniform01(key, b"phi||" + fp)
    return 2.0 * math.pi * u

@torch.no_grad()
def make_low_rank_A(key: bytes, d_embed: int, d_hid: int, rank: int, device: str) -> torch.Tensor:
    # A = B C, B:[d_embed,r], C:[r,d_hid]
    gB = seeded_torch_generator(key, b"SR_B", device=device)
    gC = seeded_torch_generator(key, b"SR_C", device=device)
    B = torch.randn(d_embed, rank, generator=gB, device=device, dtype=torch.float32) / math.sqrt(d_embed)
    C = torch.randn(rank, d_hid, generator=gC, device=device, dtype=torch.float32) / math.sqrt(d_hid)
    return B @ C  # [d_embed, d_hid]

@torch.no_grad()
def compute_w_t(w: torch.Tensor, A: torch.Tensor, h_t: torch.Tensor, lambda_couple: float) -> torch.Tensor:
    # w: [d_embed], A:[d_embed,d_hid], h_t:[d_hid]
    v = w + lambda_couple * (A @ h_t)
    return v / (v.norm() + 1e-12)

@torch.no_grad()
def pancake_score(
    s: torch.Tensor,                # [V] projection values
    omega: float,
    phi: float
) -> torch.Tensor:
    # returns [V] cosine scores
    return torch.cos(omega * s + phi)

def top_k_filter(logits: torch.Tensor, k: int) -> torch.Tensor:
    if k <= 0 or k >= logits.size(-1):
        return logits
    values, _ = torch.topk(logits, k)
    thresh = values[..., -1, None]
    return torch.where(logits < thresh, torch.full_like(logits, -1e10), logits)

def top_p_filter(logits: torch.Tensor, p: float) -> torch.Tensor:
    if p >= 1.0:
        return logits
    sorted_logits, sorted_idx = torch.sort(logits, descending=True, dim=-1)
    probs = F.softmax(sorted_logits, dim=-1)
    cum = torch.cumsum(probs, dim=-1)
    mask = cum > p
    # keep at least 1 token
    mask[..., 0] = False
    sorted_logits = sorted_logits.masked_fill(mask, -1e10)
    # scatter back
    out = torch.full_like(logits, -1e10)
    out.scatter_(dim=-1, index=sorted_idx, src=sorted_logits)
    return out

def sample_from_logits(logits: torch.Tensor, temperature: float, top_k: int, top_p: float, key: bytes, step_tag: bytes) -> int:
    # logits: [V] on device
    logits = logits / max(temperature, 1e-6)
    logits = top_k_filter(logits, top_k)
    logits = top_p_filter(logits, top_p)
    probs = F.softmax(logits, dim=-1)

    # generator must live on same device as probs
    g = seeded_torch_generator(key, b"SAMPLE||" + step_tag, device=str(probs.device))
    idx = torch.multinomial(probs, num_samples=1, generator=g)
    return int(idx.item())

@torch.no_grad()
def gpw_generate(
    prompt: str,
    key: bytes,
    decode_cfg: DecodeConfig,
    gpw_cfg: GPWConfig,
    sr_cfg: SRConfig,
    payload_cfg: PayloadConfig,
    return_debug: bool = False
) -> Tuple[str, Optional[Dict[str, Any]]]:

    enc = tokenizer(prompt, return_tensors="pt").to(device)
    input_ids = enc["input_ids"]

    E = get_token_embedding_matrix(model).to(device)
    V, d_embed = E.shape

    # Stage base direction and cached projections (for GPW/GPW-SP)
    w = derive_secret_direction_w(key, d_embed, device=device)
    s_base = precompute_projections(E, w)  # [V]

    # Semantic coupling setup (optional)
    A = None
    if sr_cfg.enabled:
        # hidden size from model config
        d_hid = model.config.n_embd if hasattr(model.config, "n_embd") else model.config.hidden_size
        A = make_low_rank_A(key, d_embed, d_hid, sr_cfg.rank, device=device)

    debug = {"scores": [], "phis": [], "tokens": []} if return_debug else None

    for t in range(decode_cfg.max_new_tokens):
        outputs = model(input_ids=input_ids, output_hidden_states=sr_cfg.enabled)
        logits = outputs.logits[0, -1, :]  # [V]

        # compute phi_t
        phi = 0.0
        if gpw_cfg.salted:
            phi = salted_phase_phi(key, input_ids, gpw_cfg)

        # choose projection s depending on SR
        if sr_cfg.enabled:
            h_t = outputs.hidden_states[-1][0, -1, :].float()  # [d_hid]
            w_t = compute_w_t(w, A, h_t, sr_cfg.lambda_couple)
            s = precompute_projections(E, w_t)  # [V]
        else:
            s = s_base

        # compute pancake score and apply logit bias
        g = pancake_score(s, gpw_cfg.omega, phi)  # [V]
        logits_wm = logits + gpw_cfg.alpha * g

        # payload extension placeholder (segment-wise phase shift)
        # We'll implement full payload encoding later; for now keep interface.
        if payload_cfg.enabled:
            seg = t // payload_cfg.segment_len
            # (placeholder) no actual bit schedule yet
            # logits_wm = logits + gpw_cfg.alpha * cos(omega*s + phi + delta_seg)
            pass

        next_id = sample_from_logits(
            logits_wm,
            temperature=decode_cfg.temperature,
            top_k=decode_cfg.top_k,
            top_p=decode_cfg.top_p,
            key=key,
            step_tag=f"t={t}".encode(),
        )

        if return_debug:
            debug["phis"].append(phi)
            debug["scores"].append(float(g[next_id].detach().cpu()))
            debug["tokens"].append(next_id)

        input_ids = torch.cat([input_ids, torch.tensor([[next_id]], device=device)], dim=1)

        # stop on EOS for GPT2? optional
        if next_id == tokenizer.eos_token_id:
            break

    text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
    return text, debug

@torch.no_grad()
def gpw_score_text(text: str, key: bytes, gpw_cfg: GPWConfig, sr_cfg: SRConfig) -> float:
    # Compute S(x)=sum_t cos(omega s_{x_t} + phi_t) under the same reconstruction
    enc = tokenizer(text, return_tensors="pt").to(device)
    input_ids = enc["input_ids"]  # [1,T]

    E = get_token_embedding_matrix(model).to(device)
    V, d_embed = E.shape
    w = derive_secret_direction_w(key, d_embed, device=device)
    s_base = precompute_projections(E, w)

    A = None
    if sr_cfg.enabled:
        d_hid = model.config.n_embd if hasattr(model.config, "n_embd") else model.config.hidden_size
        A = make_low_rank_A(key, d_embed, d_hid, sr_cfg.rank, device=device)

    S = 0.0
    # iterate token by token (skip first token because ctx undefined)
    for t in range(1, input_ids.size(1)):
        prefix = input_ids[:, :t]  # [1,t]
        token = int(input_ids[0, t].item())

        outputs = model(input_ids=prefix, output_hidden_states=sr_cfg.enabled)
        phi = salted_phase_phi(key, prefix, gpw_cfg) if gpw_cfg.salted else 0.0

        if sr_cfg.enabled:
            h_t = outputs.hidden_states[-1][0, -1, :].float()
            w_t = compute_w_t(w, A, h_t, sr_cfg.lambda_couple)
            s = precompute_projections(E, w_t)
        else:
            s = s_base

        a_t = math.cos(gpw_cfg.omega * float(s[token].detach().cpu()) + phi)
        S += a_t

    return float(S)

@torch.no_grad()
def random_key_p_value(text: str, true_key: bytes, gpw_cfg: GPWConfig, sr_cfg: SRConfig, trials: int = 200) -> float:
    obs = gpw_score_text(text, true_key, gpw_cfg, sr_cfg)
    better = 0
    for i in range(trials):
        fake_key = hashlib.sha256(b"fake||" + i.to_bytes(4,"big") + true_key).digest()
        s = gpw_score_text(text, fake_key, gpw_cfg, sr_cfg)
        if s >= obs:
            better += 1
    return (better + 1) / (trials + 1)

KEY = hashlib.sha256(b"demo-key").digest()

  decode_cfg = DecodeConfig(temperature=0.9, top_k=50, top_p=0.95, max_new_tokens=120)

  gpw_cfg = GPWConfig(alpha=1.2, omega=10.0, salted=True, ctx_mode="prev_token")
  sr_off = SRConfig(enabled=False)
  sr_on  = SRConfig(enabled=True, lambda_couple=0.2, rank=8)

  prompt = "Write a short paragraph explaining why thunderstorms happen."

  wm_text, dbg = gpw_generate(prompt, KEY, decode_cfg, gpw_cfg, sr_off, PayloadConfig(False), return_debug=True)

  # Plain sample (alpha=0)
  plain_cfg = GPWConfig(alpha=0.0, omega=gpw_cfg.omega, salted=gpw_cfg.salted, ctx_mode=gpw_cfg.ctx_mode)
  plain_text, _ = gpw_generate(prompt, KEY, decode_cfg, plain_cfg, sr_off, PayloadConfig(False), return_debug=False)

  print("=== Watermarked ===\n", wm_text[:800], "\n")
  print("=== Plain ===\n", plain_text[:800], "\n")

  wm_score = gpw_score_text(wm_text, KEY, gpw_cfg, sr_off)
  pl_score = gpw_score_text(plain_text, KEY, gpw_cfg, sr_off)
  print("Score(wm):", wm_score)
  print("Score(pl):", pl_score)
  print("Random-key p-value on wm:", random_key_p_value(wm_text, KEY, gpw_cfg, sr_off, trials=5))

"""# Experiments"""

from datasets import load_dataset

def load_prompts(n=50, max_chars=240):
    try:
        ds = load_dataset("c4", "en", split=f"train[:{n}]")
        prompts = []
        for ex in ds:
            txt = ex["text"].strip().replace("\n", " ")
            if txt:
                prompts.append(txt[:max_chars])
        return prompts[:n]
    except Exception as e:
        print("C4 load failed; using fallback prompts.", e)
        base = [
            "Explain why thunderstorms happen in simple terms.",
            "Write a short news paragraph about a local election.",
            "Summarize the pros and cons of electric cars.",
            "Write a short story opening about an astronaut waking up.",
            "Explain public-key cryptography to a beginner.",
        ]
        return (base * ((n + len(base) - 1)//len(base)))[:n]

PROMPTS = load_prompts(n=60, max_chars=240)
len(PROMPTS), PROMPTS[0]

def gen_wm_and_plain(prompt: str, key: bytes, decode_cfg: DecodeConfig, gpw_cfg: GPWConfig, sr_cfg: SRConfig):
    wm_text, _ = gpw_generate(prompt, key, decode_cfg, gpw_cfg, sr_cfg, PayloadConfig(False), return_debug=False)

    plain_cfg = GPWConfig(
        alpha=0.0,
        omega=gpw_cfg.omega,
        salted=gpw_cfg.salted,
        ctx_mode=gpw_cfg.ctx_mode,
        ngram=gpw_cfg.ngram,
    )
    plain_text, _ = gpw_generate(prompt, key, decode_cfg, plain_cfg, sr_cfg, PayloadConfig(False), return_debug=False)
    return wm_text, plain_text

def batch_generate_scores(prompts, key, decode_cfg, gpw_cfg, sr_cfg, desc=""):
    wm_texts, pl_texts = [], []
    wm_scores, pl_scores = [], []

    for p in tqdm(prompts, desc=desc):
        wt, pt = gen_wm_and_plain(p, key, decode_cfg, gpw_cfg, sr_cfg)
        wm_texts.append(wt); pl_texts.append(pt)
        wm_scores.append(gpw_score_text(wt, key, gpw_cfg, sr_cfg))
        pl_scores.append(gpw_score_text(pt, key, gpw_cfg, sr_cfg))

    return {
        "wm_texts": wm_texts,
        "pl_texts": pl_texts,
        "wm_scores": np.array(wm_scores, dtype=np.float64),
        "pl_scores": np.array(pl_scores, dtype=np.float64),
    }

def calibrate_threshold(null_scores: np.ndarray, target_fpr=0.01):
    # higher score => more watermarked in our design
    return float(np.quantile(null_scores, 1.0 - target_fpr))

def tpr_at_threshold(pos_scores: np.ndarray, thr: float):
    return float((pos_scores >= thr).mean())

def auc_from_scores(wm_scores: np.ndarray, pl_scores: np.ndarray):
    y = np.concatenate([np.ones_like(wm_scores), np.zeros_like(pl_scores)])
    s = np.concatenate([wm_scores, pl_scores])
    return float(roc_auc_score(y, s))

# Clean GPW-SP (SR off) baseline correctness
res_clean = batch_generate_scores(
    PROMPTS, KEY, decode_cfg,
    gpw_cfg=gpw_cfg,
    sr_cfg=sr_off,
    desc="C1 clean (GPW-SP, SR off)"
)

thr_1pct = calibrate_threshold(res_clean["pl_scores"], target_fpr=0.01)
tpr_1pct = tpr_at_threshold(res_clean["wm_scores"], thr_1pct)
auc = auc_from_scores(res_clean["wm_scores"], res_clean["pl_scores"])

summary = {
    "median_wm_score": float(np.median(res_clean["wm_scores"])),
    "median_plain_score": float(np.median(res_clean["pl_scores"])),
    "thr@FPR=1%": thr_1pct,
    "TPR@FPR=1%": tpr_1pct,
    "AUC": auc,
    "n": len(PROMPTS)
}
summary

# Split prompts
cal_prompts = PROMPTS[:30]
test_prompts = PROMPTS[30:]

cal_res = batch_generate_scores(cal_prompts, KEY, decode_cfg, gpw_cfg, sr_off, desc="C2 calibration split")
test_res = batch_generate_scores(test_prompts, KEY, decode_cfg, gpw_cfg, sr_off, desc="C2 test split")

thr = calibrate_threshold(cal_res["pl_scores"], target_fpr=0.01)

# Empirical FPR on held-out plain (should be ~1%)
emp_fpr = float((test_res["pl_scores"] >= thr).mean())
emp_tpr = float((test_res["wm_scores"] >= thr).mean())

{"thr@FPR=1%": thr, "empirical_FPR": emp_fpr, "empirical_TPR": emp_tpr}

lengths = [40, 80, 120, 200]

rows = []
for L in lengths:
    dc = DecodeConfig(
        temperature=decode_cfg.temperature,
        top_k=decode_cfg.top_k,
        top_p=decode_cfg.top_p,
        max_new_tokens=L
    )
    r = batch_generate_scores(
        PROMPTS[:40], KEY, dc, gpw_cfg, sr_off,
        desc=f"C3 length scaling (L={L})"
    )
    thr = calibrate_threshold(r["pl_scores"], target_fpr=0.01)
    rows.append({
        "L": L,
        "median_wm": float(np.median(r["wm_scores"])),
        "median_plain": float(np.median(r["pl_scores"])),
        "TPR@1%FPR": float((r["wm_scores"] >= thr).mean()),
        "AUC": auc_from_scores(r["wm_scores"], r["pl_scores"]),
    })

pd.DataFrame(rows)

alphas = [0.2, 0.6, 1.2]
omegas = [2.0, 5.0, 10.0]

grid_rows = []
for a in alphas:
    for w_omega in omegas:
        cfg = GPWConfig(alpha=a, omega=w_omega, salted=True, ctx_mode=gpw_cfg.ctx_mode, ngram=gpw_cfg.ngram)
        r = batch_generate_scores(PROMPTS[:30], KEY, decode_cfg, cfg, sr_off, desc=f"C4 sweep a={a}, omega={w_omega}")
        thr = calibrate_threshold(r["pl_scores"], target_fpr=0.01)
        grid_rows.append({
            "alpha": a,
            "omega": w_omega,
            "TPR@1%FPR": float((r["wm_scores"] >= thr).mean()),
            "AUC": auc_from_scores(r["wm_scores"], r["pl_scores"]),
            "median_wm": float(np.median(r["wm_scores"])),
            "median_plain": float(np.median(r["pl_scores"])),
        })

sweep_df = pd.DataFrame(grid_rows).sort_values(["TPR@1%FPR", "AUC"], ascending=False)
sweep_df

cfg_unsalted = GPWConfig(alpha=gpw_cfg.alpha, omega=gpw_cfg.omega, salted=False, ctx_mode=gpw_cfg.ctx_mode, ngram=gpw_cfg.ngram)
cfg_salted   = GPWConfig(alpha=gpw_cfg.alpha, omega=gpw_cfg.omega, salted=True,  ctx_mode=gpw_cfg.ctx_mode, ngram=gpw_cfg.ngram)

rows = []
for name, cfg in [("GPW (no salt)", cfg_unsalted), ("GPW-SP (salted)", cfg_salted)]:
    r = batch_generate_scores(PROMPTS[:40], KEY, decode_cfg, cfg, sr_off, desc=f"C5 {name}")
    thr = calibrate_threshold(r["pl_scores"], target_fpr=0.01)
    rows.append({
        "variant": name,
        "TPR@1%FPR": float((r["wm_scores"] >= thr).mean()),
        "AUC": auc_from_scores(r["wm_scores"], r["pl_scores"]),
        "median_wm": float(np.median(r["wm_scores"])),
        "median_plain": float(np.median(r["pl_scores"])),
    })

pd.DataFrame(rows)

def distinct_n(text: str, n: int = 2):
    toks = text.split()
    if len(toks) < n:
        return 0.0
    ngrams = list(zip(*[toks[i:] for i in range(n)]))
    return len(set(ngrams)) / max(1, len(ngrams))

def quality_proxy(texts: List[str]):
    d1 = np.mean([distinct_n(t, 1) for t in texts])
    d2 = np.mean([distinct_n(t, 2) for t in texts])
    avg_len = np.mean([len(t.split()) for t in texts])
    return {"distinct-1": float(d1), "distinct-2": float(d2), "avg_words": float(avg_len)}

# Use the most recent clean run (res_clean)
q_wm = quality_proxy(res_clean["wm_texts"])
q_pl = quality_proxy(res_clean["pl_texts"])

pd.DataFrame([{"kind":"watermarked", **q_wm}, {"kind":"plain", **q_pl}])

import nltk
nltk.download("wordnet")
nltk.download("omw-1.4")
from nltk.corpus import wordnet as wn

def truncate_text(text: str, keep_ratio: float = 0.5):
    toks = text.split()
    k = max(1, int(len(toks) * keep_ratio))
    return " ".join(toks[:k])

def word_delete(text: str, ratio: float, seed: int = 0):
    rng = random.Random(seed)
    toks = text.split()
    if len(toks) <= 3:
        return text
    keep = [w for w in toks if rng.random() > ratio]
    return " ".join(keep) if keep else toks[0]

def synonym_sub(text: str, ratio: float, seed: int = 0):
    rng = random.Random(seed)
    toks = text.split()
    out = []
    for w in toks:
        if rng.random() < ratio and w.isalpha() and len(w) > 3:
            syns = set()
            for syn in wn.synsets(w):
                for l in syn.lemmas():
                    cand = l.name().replace("_", " ")
                    if cand.lower() != w.lower() and cand.isalpha():
                        syns.add(cand)
            out.append(rng.choice(list(syns)) if syns else w)
        else:
            out.append(w)
    return " ".join(out)

ATTACKS = {
    "clean": lambda x, i: x,
    "truncate_50": lambda x, i: truncate_text(x, 0.50),
    "delete_10": lambda x, i: word_delete(x, 0.10, seed=GLOBAL_SEED+i),
    "delete_20": lambda x, i: word_delete(x, 0.20, seed=GLOBAL_SEED+i),
    "syn_10": lambda x, i: synonym_sub(x, 0.10, seed=GLOBAL_SEED+i),
    "syn_20": lambda x, i: synonym_sub(x, 0.20, seed=GLOBAL_SEED+i),
}

# Calibrate threshold on clean plain
thr = calibrate_threshold(res_clean["pl_scores"], target_fpr=0.01)

rob_rows = []
for atk_name, atk_fn in ATTACKS.items():
    scores = []
    for i, txt in enumerate(res_clean["wm_texts"]):
        attacked = atk_fn(txt, i)
        scores.append(gpw_score_text(attacked, KEY, gpw_cfg, sr_off))
    scores = np.array(scores, dtype=np.float64)
    rob_rows.append({
        "attack": atk_name,
        "TPR@1%FPR (using clean threshold)": float((scores >= thr).mean()),
        "median_score": float(np.median(scores)),
    })

pd.DataFrame(rob_rows)

os.makedirs("starter_results", exist_ok=True)

pd.DataFrame([summary]).to_csv("starter_results/clean_summary.csv", index=False)
pd.DataFrame(rows).to_csv("starter_results/length_scaling.csv", index=False)
sweep_df.to_csv("starter_results/alpha_omega_sweep.csv", index=False)

print("Saved starter tables in starter_results/:")
print(" - clean_summary.csv")
print(" - length_scaling.csv")
print(" - alpha_omega_sweep.csv")

"""# Synonym and Paraphrase Attack"""

from datasets import load_dataset

def load_prompts(n=60, seed=1234):
    ds = load_dataset("truthful_qa", "generation", split="validation")
    qs = ds["question"]
    rng = random.Random(seed)
    idx = list(range(len(qs)))
    rng.shuffle(idx)
    return [qs[i] for i in idx[:n]]

prompts = load_prompts(n=60, seed=GLOBAL_SEED)
prompts[:3]

!pip -q install nltk
import re
import nltk
nltk.download("wordnet")
nltk.download("omw-1.4")

from nltk.corpus import wordnet as wn

_WORD_RE = re.compile(r"\b[\w']+\b")

# Minimal stopword set (kept small to avoid heavy deps)
STOPWORDS = set("""
a an the and or but if then else for with without to of in on at by from as is are was were be been being
this that these those it its i you he she they we my your our their
""".split())

def _get_wordnet_synonym(word: str):
    w = word.lower()
    syns = set()
    for s in wn.synsets(w):
        for lemma in s.lemmas():
            cand = lemma.name().replace("_", " ")
            if cand.lower() != w and cand.isalpha():
                syns.add(cand)
    if not syns:
        return None
    # choose the shortest synonym (often most natural); could randomize later
    return sorted(syns, key=len)[0]

def synonym_replace(text: str, rate: float, seed: int = 0) -> str:
    """
    Replace ~rate fraction of eligible words with WordNet synonyms.
    Keeps punctuation/spacing by doing span-based replacement.
    """
    rng = random.Random(seed)
    matches = list(_WORD_RE.finditer(text))
    if not matches:
        return text

    eligible = []
    for mi, m in enumerate(matches):
        w = m.group(0)
        if w.lower() in STOPWORDS:
            continue
        if not w.isalpha():
            continue
        syn = _get_wordnet_synonym(w)
        if syn is None:
            continue
        eligible.append((mi, syn))

    if not eligible:
        return text

    k = int(round(rate * len(eligible)))
    k = max(1, k) if rate > 0 else 0
    k = min(k, len(eligible))
    chosen = rng.sample(eligible, k=k)

    # build replacement map by match index
    rep = {mi: syn for (mi, syn) in chosen}

    out = []
    last = 0
    for mi, m in enumerate(matches):
        out.append(text[last:m.start()])
        w = m.group(0)
        if mi in rep:
            syn = rep[mi]
            # preserve capitalization crudely
            if w[0].isupper():
                syn = syn.capitalize()
            out.append(syn)
        else:
            out.append(w)
        last = m.end()
    out.append(text[last:])
    return "".join(out)

# quick sanity check
t = "Thunderstorms happen when warm moist air rises rapidly and cools, forming clouds and lightning."
print(synonym_replace(t, rate=0.3, seed=1))

from transformers import AutoModelForSeq2SeqLM

PARA_MODEL = "ramsrigouthamg/t5_paraphraser"  # lightweight paraphraser
para_tok = AutoTokenizer.from_pretrained(PARA_MODEL)
para_model = AutoModelForSeq2SeqLM.from_pretrained(PARA_MODEL).to(device)
para_model.eval()

@torch.no_grad()
def paraphrase_t5(text: str, seed: int = 0, max_new_tokens: int = 128) -> str:
    """
    Paraphrase with a T5 paraphraser. Deterministic-ish via seed.
    """
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

    inp = "paraphrase: " + text + " </s>"
    enc = para_tok([inp], return_tensors="pt", truncation=True).to(device)

    # beam search tends to preserve meaning better than sampling
    out_ids = para_model.generate(
        **enc,
        num_beams=5,
        do_sample=False,
        max_new_tokens=max_new_tokens,
        early_stopping=True,
    )
    out = para_tok.decode(out_ids[0], skip_special_tokens=True).strip()
    return out if len(out) > 0 else text

# quick sanity check
print(paraphrase_t5(t, seed=2))

def calibrate_thr(null_scores, target_fpr=0.01, higher_is_more=True):
    null_scores = np.array([s for s in null_scores if s is not None], dtype=np.float64)
    if len(null_scores) < 10:
        return None
    q = 1.0 - target_fpr if higher_is_more else target_fpr
    return float(np.quantile(null_scores, q))

def auc_and_tpr(wm_scores, null_scores, thr, higher_is_more=True):
    wm = np.array(wm_scores, dtype=np.float64)
    nu = np.array(null_scores, dtype=np.float64)

    if higher_is_more:
        tpr = float((wm >= thr).mean())
        y = np.concatenate([np.ones_like(wm), np.zeros_like(nu)])
        s = np.concatenate([wm, nu])
    else:
        tpr = float((wm <= thr).mean())
        y = np.concatenate([np.ones_like(wm), np.zeros_like(nu)])
        s = np.concatenate([-wm, -nu])

    auc = float(roc_auc_score(y, s))
    return auc, tpr

def run_attack_eval(
    prompts,
    attack_name: str,
    attack_fn,                 # text -> text
    gpw_cfg: GPWConfig,
    sr_cfg: SRConfig,
    decode_cfg: DecodeConfig,
    target_fpr: float = 0.01,
    max_items: int = None,
):
    wm_scores, null_scores = [], []
    examples = []

    use_prompts = prompts[:max_items] if max_items is not None else prompts

    plain_cfg = GPWConfig(
        alpha=0.0,
        omega=gpw_cfg.omega,
        salted=gpw_cfg.salted,
        ctx_mode=gpw_cfg.ctx_mode,
        ngram=gpw_cfg.ngram
    )

    for i, p in enumerate(tqdm(use_prompts, desc=f"[{attack_name}]")):
        wm_text, _ = gpw_generate(p, KEY, decode_cfg, gpw_cfg, sr_cfg, PayloadConfig(False), return_debug=False)
        pl_text, _ = gpw_generate(p, KEY, decode_cfg, plain_cfg, sr_cfg, PayloadConfig(False), return_debug=False)

        wm_att = attack_fn(wm_text, i)
        pl_att = attack_fn(pl_text, i + 10_000)

        sw = gpw_score_text(wm_att, KEY, gpw_cfg, sr_cfg)
        su = gpw_score_text(pl_att, KEY, gpw_cfg, sr_cfg)

        wm_scores.append(sw)
        null_scores.append(su)

        if len(examples) < 2:
            examples.append((p, wm_text[:250], wm_att[:250], pl_text[:250], pl_att[:250], sw, su))

    # direction: our score should increase under watermark
    higher_is_more = (np.median(wm_scores) >= np.median(null_scores))
    thr = calibrate_thr(null_scores, target_fpr=target_fpr, higher_is_more=higher_is_more)

    auc, tpr = (np.nan, np.nan)
    if thr is not None:
        auc, tpr = auc_and_tpr(wm_scores, null_scores, thr, higher_is_more=higher_is_more)

    row = {
        "attack": attack_name,
        "salted": gpw_cfg.salted,
        "SR": sr_cfg.enabled,
        "alpha": gpw_cfg.alpha,
        "omega": gpw_cfg.omega,
        "AUC": auc,
        f"TPR@{int(target_fpr*100)}%FPR": tpr,
        "thr": thr,
        "median_wm": float(np.median(wm_scores)),
        "median_plain": float(np.median(null_scores)),
        "n": len(use_prompts),
    }
    return row, examples

def no_attack(text, seed):
    return text

def syn_attack_factory(rate):
    return lambda text, seed: synonym_replace(text, rate=rate, seed=seed)

def para_attack(text, seed):
    return paraphrase_t5(text, seed=seed, max_new_tokens=128)

rows = []

# pick one configuration as a baseline
base_decode = decode_cfg
base_gpw = gpw_cfg
base_sr = sr_off  # start with SR disabled

# 1) clean
r, ex = run_attack_eval(prompts, "clean", no_attack, base_gpw, base_sr, base_decode, target_fpr=0.01)
rows.append(r)
print("Example (clean):", ex[0][-2:])

# 2) UNIGRAM-style synonym replacement rates
for rate in [0.1, 0.3, 0.5]:
    r, ex = run_attack_eval(prompts, f"syn_replace_{rate}", syn_attack_factory(rate), base_gpw, base_sr, base_decode, target_fpr=0.01)
    rows.append(r)

# 3) paraphrase (proxy for UNIGRAM ChatGPT/DIPPER and REMARK rephrase)
r, ex = run_attack_eval(prompts, "paraphrase_t5", para_attack, base_gpw, base_sr, base_decode, target_fpr=0.01)
rows.append(r)

pd.DataFrame(rows).sort_values("AUC", ascending=False)

"""### Salting and semantic salting"""

# --- Variant grid (methodology ablations) ---
ALPHA = 1.2
OMEGA = 10.0

cfg_gpw_static = GPWConfig(alpha=ALPHA, omega=OMEGA, salted=False, ctx_mode=gpw_cfg.ctx_mode, ngram=gpw_cfg.ngram)
cfg_gpw_sp     = GPWConfig(alpha=ALPHA, omega=OMEGA, salted=True,  ctx_mode=gpw_cfg.ctx_mode, ngram=gpw_cfg.ngram)

VARIANTS = [
    ("GPW (no salt)",        cfg_gpw_static, sr_off),
    ("GPW-SP (salted)",      cfg_gpw_sp,     sr_off),
    ("GPW-SP+SR (semantic)", cfg_gpw_sp,     sr_on),
]

TARGET_FPR = 0.01
MAX_ITEMS = 20  # set smaller (e.g., 30) if you want faster

# --- Fix ATTACKS shape: enforce list of (name, fn) pairs ---

def _normalize_attacks(attacks_obj):
    # Case 1: dict {name: fn}
    if isinstance(attacks_obj, dict):
        return [(k, v) for k, v in attacks_obj.items()]

    # Case 2: list/tuple of items
    out = []
    for item in attacks_obj:
        # item could be (name, fn) or (name, fn, meta...)
        if isinstance(item, (list, tuple)):
            if len(item) < 2:
                raise ValueError(f"Attack entry has <2 elements: {item}")
            name, fn = item[0], item[1]
            out.append((name, fn))
        else:
            raise ValueError(f"Attack entry is not tuple/list: {item}")
    return out

# Rebuild ATTACKS explicitly (recommended) to avoid notebook-state surprises:
ATTACKS = [
    ("clean", lambda text, seed: text),
    ("syn_replace_0.1", lambda text, seed: synonym_replace(text, rate=0.1, seed=seed)),
    ("syn_replace_0.3", lambda text, seed: synonym_replace(text, rate=0.3, seed=seed)),
    ("syn_replace_0.5", lambda text, seed: synonym_replace(text, rate=0.5, seed=seed)),
    ("paraphrase_t5", lambda text, seed: paraphrase_t5(text, seed=seed, max_new_tokens=128)),
]

# If you prefer to keep your existing ATTACKS object, uncomment:
# ATTACKS = _normalize_attacks(ATTACKS)

print("ATTACKS entries:", len(ATTACKS))
print("Sample entry:", ATTACKS[0], "->", type(ATTACKS[0]), "len=", len(ATTACKS[0]))

suite_rows = []
suite_examples = {}

for vname, vcfg, vsr in VARIANTS:
    for aname, afn in ATTACKS:
        row, examples = run_attack_eval(
            prompts,
            attack_name=aname,
            attack_fn=afn,
            gpw_cfg=vcfg,
            sr_cfg=vsr,
            decode_cfg=decode_cfg,
            target_fpr=TARGET_FPR,
            max_items=MAX_ITEMS,
        )
        row["variant"] = vname
        suite_rows.append(row)

        # keep one example bundle per variant+attack
        if examples and (vname, aname) not in suite_examples:
            suite_examples[(vname, aname)] = examples[0]

suite_df = pd.DataFrame(suite_rows)

# reorder columns nicely
cols = ["variant", "attack", "salted", "SR", "alpha", "omega", "AUC", f"TPR@{int(TARGET_FPR*100)}%FPR",
        "thr", "median_wm", "median_plain", "n"]
suite_df = suite_df[cols].sort_values(["variant", "attack"])
suite_df

"""### Inference for attacks."""

import json, os
from datetime import datetime

def make_runs_for_prompts(
    prompts: List[str],
    n_per_prompt: int,
    variant_name: str,
    gpw_cfg: GPWConfig,
    sr_cfg: SRConfig,
    decode_cfg: DecodeConfig,
    key: bytes,
    out_path: str,
):
    """
    Writes JSONL with one row per generation:
    {
      "variant": ...,
      "prompt_id": ...,
      "gen_id": ...,
      "prompt": ...,
      "text": ...,
      "score": ...,
      "meta": {...}
    }
    """
    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    rows = []
    for pid, p in enumerate(tqdm(prompts, desc=f"Generate {variant_name}")):
        for gid in range(n_per_prompt):
            # Make each run slightly different by perturbing sampling seed tag via KEY+gid+pid
            # (gpw_generate already uses per-step seeded sampling; this just changes internal randomness path)
            run_key = hashlib.sha256(key + f"|pid={pid}|gid={gid}".encode()).digest()

            text, _ = gpw_generate(
                prompt=p,
                key=run_key,
                decode_cfg=decode_cfg,
                gpw_cfg=gpw_cfg,
                sr_cfg=sr_cfg,
                payload_cfg=PayloadConfig(False),
                return_debug=False,
            )
            score = gpw_score_text(text, run_key, gpw_cfg, sr_cfg)

            rows.append({
                "variant": variant_name,
                "prompt_id": pid,
                "gen_id": gid,
                "prompt": p,
                "text": text,
                "score": float(score),
                "meta": {
                    "model": MODEL_NAME,
                    "decode": decode_cfg.__dict__,
                    "gpw": gpw_cfg.__dict__,
                    "sr": sr_cfg.__dict__,
                    "timestamp": datetime.utcnow().isoformat() + "Z",
                }
            })

    # Write JSONL
    with open(out_path, "w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

    return rows

# ---- SETTINGS ----
N_PER_PROMPT = 10
OUT_DIR = "wm_outputs_roundtrip"
os.makedirs(OUT_DIR, exist_ok=True)

# Use the salted configs you already defined earlier:
# cfg_gpw_sp, sr_off, sr_on

# If those names don't exist in your notebook, uncomment:
# cfg_gpw_sp = GPWConfig(alpha=1.2, omega=10.0, salted=True, ctx_mode=gpw_cfg.ctx_mode, ngram=gpw_cfg.ngram)

rows_sp = make_runs_for_prompts(
    prompts=prompts,
    n_per_prompt=N_PER_PROMPT,
    variant_name="GPW-SP",
    gpw_cfg=cfg_gpw_sp,
    sr_cfg=sr_off,
    decode_cfg=decode_cfg,
    key=KEY,
    out_path=os.path.join(OUT_DIR, "gpw_sp_generations.jsonl"),
)

rows_srs = make_runs_for_prompts(
    prompts=prompts,
    n_per_prompt=N_PER_PROMPT,
    variant_name="GPW-SP+SR",
    gpw_cfg=cfg_gpw_sp,
    sr_cfg=sr_on,
    decode_cfg=decode_cfg,
    key=KEY,
    out_path=os.path.join(OUT_DIR, "gpw_sp_sr_generations.jsonl"),
)

print("Saved:")
print(" -", os.path.join(OUT_DIR, "gpw_sp_generations.jsonl"))
print(" -", os.path.join(OUT_DIR, "gpw_sp_sr_generations.jsonl"))

# Quick preview: first 2 rows from each
print("\nPreview GPW-SP:")
for r in rows_sp[:2]:
    print("score=", round(r["score"], 3), "| text[:120]=", r["text"][:120].replace("\n"," "))

print("\nPreview GPW-SP+SR:")
for r in rows_srs[:2]:
    print("score=", round(r["score"], 3), "| text[:120]=", r["text"][:120].replace("\n"," "))

!pip -q install transformers accelerate sentencepiece datasets evaluate scikit-learn nltk

import os, math, hashlib, random, time, json
from dataclasses import dataclass
from typing import Optional, Dict, Any, List, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM

from sklearn.metrics import roc_auc_score

# Repro
GLOBAL_SEED = 1234
random.seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)
torch.manual_seed(GLOBAL_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(GLOBAL_SEED)

device = "cuda" if torch.cuda.is_available() else "cpu"
print("device:", device)

MODEL_NAME = "gpt2"   # keep starter as-is
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)
model.eval()

# GPT-2 has no pad token by default
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Canonical key for starter tables
KEY = hashlib.sha256(b"gpwsp-starter-key").digest()

from datasets import load_dataset

PROMPT_SOURCE = "truthfulqa"   # "truthfulqa" or "c4"
N_PROMPTS = 80                # increase later if you want (e.g., 200)
MAX_CHARS = 240

def load_prompts_truthfulqa(n=80, seed=1234):
    ds = load_dataset("truthful_qa", "generation", split="validation")
    qs = list(ds["question"])
    rng = random.Random(seed)
    rng.shuffle(qs)
    return qs[:n]

def load_prompts_c4(n=80, seed=1234, max_chars=240):
    ds = load_dataset("c4", "en", split=f"train[:{max(5*n, n)}]")
    texts = []
    for ex in ds:
        txt = ex["text"].strip().replace("\n", " ")
        if txt:
            texts.append(txt[:max_chars])
        if len(texts) >= n:
            break
    # shuffle deterministically
    rng = random.Random(seed)
    rng.shuffle(texts)
    return texts[:n]

if PROMPT_SOURCE == "truthfulqa":
    PROMPTS = load_prompts_truthfulqa(N_PROMPTS, seed=GLOBAL_SEED)
else:
    PROMPTS = load_prompts_c4(N_PROMPTS, seed=GLOBAL_SEED, max_chars=MAX_CHARS)

print("Loaded prompts:", len(PROMPTS), "source:", PROMPT_SOURCE)
print("Example:", PROMPTS[0][:120])

# ---- These helpers assume your PRF helpers exist:
# _hmac_like, prf_to_uniform01, seeded_torch_generator
# and your configs exist: GPWConfig, SRConfig
# If not, paste your watermark code cell above this one.

@torch.no_grad()
def _get_E(model) -> torch.Tensor:
    return model.get_input_embeddings().weight.detach()  # [V, d]

@torch.no_grad()
def _derive_w(key: bytes, d: int, device: str) -> torch.Tensor:
    g = seeded_torch_generator(key, b"GPW_w", device=device)
    v = torch.randn(d, generator=g, device=device, dtype=torch.float32)
    return v / (v.norm() + 1e-12)

def _ctx_fp_from_ids(ids: List[int], mode: str, ngram: int) -> bytes:
    if len(ids) == 0:
        return b"empty"
    if mode == "prev_token":
        return f"prev:{ids[-1]}".encode()
    if mode == "ngram":
        tail = ids[-ngram:] if len(ids) >= ngram else ids
        return ("ngram:" + ",".join(map(str, tail))).encode()
    if mode == "rolling":
        h = hashlib.sha256(("roll:" + ",".join(map(str, ids[-64:]))).encode()).digest()
        return b"roll:" + h[:16]
    raise ValueError(f"Unknown ctx_mode={mode}")

def _phi_from_prefix_ids(key: bytes, prefix_ids: List[int], gpw_cfg) -> float:
    fp = _ctx_fp_from_ids(prefix_ids, gpw_cfg.ctx_mode, gpw_cfg.ngram)
    u = prf_to_uniform01(key, b"phi||" + fp)
    return 2.0 * math.pi * u

class GPWPrecomp:
    def __init__(self, model, key: bytes, device: str):
        E = _get_E(model).to(device)             # [V, d]
        V, d = E.shape
        w = _derive_w(key, d, device=device)     # [d]
        s_base = (E @ w).float()                 # [V]
        self.E = E
        self.V = V
        self.d = d
        self.w = w
        self.s_base = s_base

@torch.no_grad()
def gpw_score_text_fast(
    text: str,
    key: bytes,
    gpw_cfg,
    precomp: GPWPrecomp,
) -> float:
    # SR disabled: no model forward needed
    enc = tokenizer(text, return_tensors="pt", truncation=True).input_ids[0].tolist()
    if len(enc) <= 1:
        return 0.0

    S = 0.0
    for t in range(1, len(enc)):
        token = enc[t]
        prefix = enc[:t]  # ids up to t-1
        phi = _phi_from_prefix_ids(key, prefix, gpw_cfg) if gpw_cfg.salted else 0.0
        s_tok = float(precomp.s_base[token].detach().cpu())
        S += math.cos(gpw_cfg.omega * s_tok + phi)
    return float(S)

@torch.no_grad()
def make_low_rank_A_factors(key: bytes, d_embed: int, d_hid: int, rank: int, device: str):
    # A = B C, B:[d_embed,r], C:[r,d_hid]
    gB = seeded_torch_generator(key, b"SR_B", device=device)
    gC = seeded_torch_generator(key, b"SR_C", device=device)
    B = torch.randn(d_embed, rank, generator=gB, device=device, dtype=torch.float32) / math.sqrt(d_embed)
    C = torch.randn(rank, d_hid, generator=gC, device=device, dtype=torch.float32) / math.sqrt(d_hid)
    return B, C

@torch.no_grad()
def gpw_score_text_fast_sr(
    text: str,
    key: bytes,
    gpw_cfg,
    sr_cfg,
    precomp: GPWPrecomp,
) -> float:
    # SR enabled: one model forward pass + O(T*rank) math
    enc = tokenizer(text, return_tensors="pt", truncation=True).to(device)
    input_ids = enc["input_ids"]  # [1, T]
    T = input_ids.size(1)
    if T <= 1:
        return 0.0

    outputs = model(input_ids=input_ids, output_hidden_states=True)
    # hidden at each position i corresponds to prefix up to i
    h = outputs.hidden_states[-1][0].float()  # [T, d_hid]

    d_hid = h.size(-1)
    rank = sr_cfg.rank

    # low-rank A factors, precompute EB = E@B
    B, C = make_low_rank_A_factors(key, precomp.d, d_hid, rank, device=device)
    EB = (precomp.E @ B).float()  # [V, rank]

    S = 0.0
    ids = input_ids[0].tolist()
    for t in range(1, T):
        token = ids[t]
        prefix = ids[:t]
        phi = _phi_from_prefix_ids(key, prefix, gpw_cfg) if gpw_cfg.salted else 0.0

        # u = C @ h_{t-1}  (note: use hidden state at last token of prefix)
        u = (C @ h[t-1]).float()  # [rank]
        # s_token = s_base[token] + lambda * (EB[token] @ u)
        s_tok = float(precomp.s_base[token].detach().cpu()) + float(sr_cfg.lambda_couple) * float((EB[token] @ u).detach().cpu())
        # normalize w_t doesn't matter for single-token score when using this linear form;
        # it's a good approximation for detection. (If you want exact normalization, add it later.)
        S += math.cos(gpw_cfg.omega * s_tok + phi)
    return float(S)

print("Fast scoring ready.")

# Configs (match your draft)
decode_cfg = DecodeConfig(temperature=0.9, top_k=50, top_p=0.95, max_new_tokens=120)

gpw_sp = GPWConfig(alpha=1.2, omega=10.0, salted=True, ctx_mode="prev_token", ngram=4)
gpw_nosalt = GPWConfig(alpha=1.2, omega=10.0, salted=False, ctx_mode="prev_token", ngram=4)

sr_off = SRConfig(enabled=False)
sr_on  = SRConfig(enabled=True, lambda_couple=0.2, rank=8)

# Precompute for fast detection (SR-off uses this directly; SR-on uses it too)
precomp = GPWPrecomp(model, KEY, device=device)

print("Configs ready.")

def calibrate_threshold(null_scores: np.ndarray, target_fpr=0.01) -> float:
    return float(np.quantile(null_scores, 1.0 - target_fpr))

def auc_from_scores(pos_scores: np.ndarray, null_scores: np.ndarray) -> float:
    y = np.concatenate([np.ones_like(pos_scores), np.zeros_like(null_scores)])
    s = np.concatenate([pos_scores, null_scores])
    return float(roc_auc_score(y, s))

@torch.no_grad()
def generate_pair(prompt: str, key: bytes, decode_cfg, wm_cfg, sr_cfg):
    # Watermarked
    wm_text, _ = gpw_generate(prompt, key, decode_cfg, wm_cfg, sr_cfg, PayloadConfig(False), return_debug=False)
    # Plain: alpha = 0
    plain_cfg = GPWConfig(alpha=0.0, omega=wm_cfg.omega, salted=wm_cfg.salted, ctx_mode=wm_cfg.ctx_mode, ngram=wm_cfg.ngram)
    pl_text, _ = gpw_generate(prompt, key, decode_cfg, plain_cfg, sr_cfg, PayloadConfig(False), return_debug=False)
    return wm_text, pl_text

def score_text(text: str, key: bytes, gpw_cfg, sr_cfg):
    if not sr_cfg.enabled:
        return gpw_score_text_fast(text, key, gpw_cfg, precomp)
    else:
        return gpw_score_text_fast_sr(text, key, gpw_cfg, sr_cfg, precomp)

def run_clean_eval(prompts, key: bytes, decode_cfg, gpw_cfg, sr_cfg, target_fpr=0.01, max_items=None, desc=""):
    use_prompts = prompts[:max_items] if max_items is not None else prompts
    wm_scores, pl_scores = [], []
    for p in tqdm(use_prompts, desc=desc):
        wm_text, pl_text = generate_pair(p, key, decode_cfg, gpw_cfg, sr_cfg)
        wm_scores.append(score_text(wm_text, key, gpw_cfg, sr_cfg))
        pl_scores.append(score_text(pl_text, key, gpw_cfg, sr_cfg))
    wm_scores = np.array(wm_scores, dtype=np.float64)
    pl_scores = np.array(pl_scores, dtype=np.float64)

    thr = calibrate_threshold(pl_scores, target_fpr=target_fpr)
    tpr = float((wm_scores >= thr).mean())
    auc = auc_from_scores(wm_scores, pl_scores)

    out = {
        "n": len(use_prompts),
        "median_wm": float(np.median(wm_scores)),
        "median_plain": float(np.median(pl_scores)),
        "thr@FPR": float(thr),
        "TPR@FPR": tpr,
        "AUC": auc,
    }
    return out

@torch.no_grad()
def gpw_score_text_sr_exact(
    text: str,
    key: bytes,
    gpw_cfg: GPWConfig,
    sr_cfg: SRConfig,
    precomp: GPWPrecomp,
) -> float:
    """
    Exact per-token SR score consistent with generation:
      w_t = norm(w + lambda * (A @ h_t))
      s_tok = <E[token], w_t>
      a_t = cos(omega * s_tok + phi_t)
    Uses A = B@C with low rank; computes (A @ h) as B @ (C @ h).
    """
    enc = tokenizer(text, return_tensors="pt", truncation=True).to(device)
    input_ids = enc["input_ids"]  # [1, T]
    T = input_ids.size(1)
    if T <= 1:
        return 0.0

    # One forward pass for all hidden states
    out = model(input_ids=input_ids, output_hidden_states=True)
    h = out.hidden_states[-1][0].float()  # [T, d_hid]

    d_hid = h.size(-1)
    B, C = make_low_rank_A_factors(key, precomp.d, d_hid, sr_cfg.rank, device=device)

    ids = input_ids[0].tolist()
    S = 0.0

    # iterate tokens x_t for t=1..T-1, prefix ends at t-1
    for t in range(1, T):
        token = ids[t]
        prefix_ids = ids[:t]

        phi = _phi_from_prefix_ids(key, prefix_ids, gpw_cfg) if gpw_cfg.salted else 0.0

        # h_t corresponds to last token of prefix => position t-1
        u = (C @ h[t-1]).float()                # [rank]
        delta = (B @ u).float()                 # [d_embed]
        v = (precomp.w + sr_cfg.lambda_couple * delta).float()
        v = v / (v.norm() + 1e-12)

        e_tok = precomp.E[token].float()        # [d_embed]
        s_tok = float((e_tok @ v).detach().cpu())

        S += math.cos(gpw_cfg.omega * s_tok + phi)

    return float(S)

def score_text(text: str, key: bytes, gpw_cfg, sr_cfg):
    if not sr_cfg.enabled:
        return gpw_score_text_fast(text, key, gpw_cfg, precomp)
    else:
        return gpw_score_text_sr_exact(text, key, gpw_cfg, sr_cfg, precomp)

os.makedirs("paper_results", exist_ok=True)

# --- Clean (GPW-SP) ---
clean_sp = run_clean_eval(PROMPTS, KEY, decode_cfg, gpw_sp, sr_off, target_fpr=0.01, desc="Clean GPW-SP (SR off)")
print("CLEAN GPW-SP:", clean_sp)

# --- Salt ablation ---
clean_nosalt = run_clean_eval(PROMPTS, KEY, decode_cfg, gpw_nosalt, sr_off, target_fpr=0.01, desc="Clean GPW (no salt, SR off)")
print("CLEAN GPW no-salt:", clean_nosalt)

salt_df = pd.DataFrame([
    {"variant": "GPW (no salt)", **clean_nosalt},
    {"variant": "GPW-SP (salted)", **clean_sp},
])
salt_df.to_csv("paper_results/starter_salt_ablation.csv", index=False)
salt_df

import re
import nltk
nltk.download("wordnet")
nltk.download("omw-1.4")
from nltk.corpus import wordnet as wn

_WORD_RE = re.compile(r"\b[\w']+\b")
STOPWORDS = set("""
a an the and or but if then else for with without to of in on at by from as is are was were be been being
this that these those it its i you he she they we my your our their
""".split())

def _get_wordnet_synonym(word: str):
    w = word.lower()
    syns = set()
    for s in wn.synsets(w):
        for lemma in s.lemmas():
            cand = lemma.name().replace("_", " ")
            if cand.lower() != w and cand.isalpha():
                syns.add(cand)
    if not syns:
        return None
    return sorted(syns, key=len)[0]

def synonym_replace(text: str, rate: float, seed: int = 0) -> str:
    rng = random.Random(seed)
    matches = list(_WORD_RE.finditer(text))
    if not matches:
        return text

    eligible = []
    for mi, m in enumerate(matches):
        w = m.group(0)
        if w.lower() in STOPWORDS:
            continue
        if not w.isalpha():
            continue
        syn = _get_wordnet_synonym(w)
        if syn is None:
            continue
        eligible.append((mi, syn))

    if not eligible or rate <= 0:
        return text

    k = int(round(rate * len(eligible)))
    k = max(1, k)
    k = min(k, len(eligible))
    chosen = rng.sample(eligible, k=k)
    rep = {mi: syn for (mi, syn) in chosen}

    out = []
    last = 0
    for mi, m in enumerate(matches):
        out.append(text[last:m.start()])
        w = m.group(0)
        if mi in rep:
            syn = rep[mi]
            if w[0].isupper():
                syn = syn.capitalize()
            out.append(syn)
        else:
            out.append(w)
        last = m.end()
    out.append(text[last:])
    return "".join(out)

def run_attack_curve_syn(prompts, key, decode_cfg, wm_cfg, sr_cfg, rates, target_fpr=0.01, max_items=40):
    rows = []
    use_prompts = prompts[:max_items]
    for r in rates:
        wm_scores, nu_scores = [], []
        for i, p in enumerate(tqdm(use_prompts, desc=f"syn_rate={r}")):
            wm_text, pl_text = generate_pair(p, key, decode_cfg, wm_cfg, sr_cfg)
            wm_att = synonym_replace(wm_text, rate=r, seed=GLOBAL_SEED + i)
            pl_att = synonym_replace(pl_text, rate=r, seed=GLOBAL_SEED + 10_000 + i)
            wm_scores.append(score_text(wm_att, key, wm_cfg, sr_cfg))
            nu_scores.append(score_text(pl_att, key, wm_cfg, sr_cfg))

        wm_scores = np.array(wm_scores, dtype=np.float64)
        nu_scores = np.array(nu_scores, dtype=np.float64)
        thr = calibrate_threshold(nu_scores, target_fpr=target_fpr)
        rows.append({
            "attack": "synonym_replace",
            "strength": r,
            "AUC": auc_from_scores(wm_scores, nu_scores),
            f"TPR@{int(target_fpr*100)}%FPR": float((wm_scores >= thr).mean()),
            "thr": float(thr),
            "median_wm": float(np.median(wm_scores)),
            "median_null": float(np.median(nu_scores)),
            "n": len(use_prompts),
        })
    return pd.DataFrame(rows)

rates = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]
syn_curve_df = run_attack_curve_syn(PROMPTS, KEY, decode_cfg, gpw_sp, sr_off, rates, target_fpr=0.01, max_items=40)
syn_curve_df.to_csv("paper_results/robustness_curve_synonym.csv", index=False)
syn_curve_df

from transformers import AutoModelForSeq2SeqLM

PARA_MODEL = "ramsrigouthamg/t5_paraphraser"
para_tok = AutoTokenizer.from_pretrained(PARA_MODEL)
para_model = AutoModelForSeq2SeqLM.from_pretrained(PARA_MODEL).to(device)
para_model.eval()

@torch.no_grad()
def paraphrase_t5(text: str, seed: int = 0, num_beams: int = 5, do_sample: bool = False, max_new_tokens: int = 128) -> str:
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

    inp = "paraphrase: " + text + " </s>"
    enc = para_tok([inp], return_tensors="pt", truncation=True).to(device)

    out_ids = para_model.generate(
        **enc,
        num_beams=num_beams,
        do_sample=do_sample,
        max_new_tokens=max_new_tokens,
        early_stopping=True,
    )
    out = para_tok.decode(out_ids[0], skip_special_tokens=True).strip()
    return out if out else text

def run_paraphrase_strengths(prompts, key, decode_cfg, wm_cfg, sr_cfg, strengths, target_fpr=0.01, max_items=20):
    rows = []
    use_prompts = prompts[:max_items]
    for (name, kwargs) in strengths:
        wm_scores, nu_scores = [], []
        for i, p in enumerate(tqdm(use_prompts, desc=f"paraphrase={name}")):
            wm_text, pl_text = generate_pair(p, key, decode_cfg, wm_cfg, sr_cfg)
            wm_att = paraphrase_t5(wm_text, seed=GLOBAL_SEED + i, **kwargs)
            pl_att = paraphrase_t5(pl_text, seed=GLOBAL_SEED + 10_000 + i, **kwargs)
            wm_scores.append(score_text(wm_att, key, wm_cfg, sr_cfg))
            nu_scores.append(score_text(pl_att, key, wm_cfg, sr_cfg))

        wm_scores = np.array(wm_scores, dtype=np.float64)
        nu_scores = np.array(nu_scores, dtype=np.float64)
        thr = calibrate_threshold(nu_scores, target_fpr=target_fpr)
        rows.append({
            "attack": "paraphrase_t5",
            "strength": name,
            "AUC": auc_from_scores(wm_scores, nu_scores),
            f"TPR@{int(target_fpr*100)}%FPR": float((wm_scores >= thr).mean()),
            "thr": float(thr),
            "median_wm": float(np.median(wm_scores)),
            "median_null": float(np.median(nu_scores)),
            "n": len(use_prompts),
        })
    return pd.DataFrame(rows)

strengths = [
    ("beam5", {"num_beams": 5, "do_sample": False}),
    ("beam1", {"num_beams": 1, "do_sample": False}),  # weaker rewrite, often closer to input
]

#para_df = run_paraphrase_strengths(PROMPTS, KEY, decode_cfg, gpw_sp, sr_off, strengths, target_fpr=0.01, max_items=20)
#para_df.to_csv("paper_results/robustness_paraphrase.csv", index=False)
#para_df

def eval_variant_under_attack(prompts, key, decode_cfg, wm_cfg, sr_cfg, attack_name, attack_fn, target_fpr=0.01, max_items=20):
    wm_scores, nu_scores = [], []
    use_prompts = prompts[:max_items]
    for i, p in enumerate(tqdm(use_prompts, desc=f"{attack_name}")):
        wm_text, pl_text = generate_pair(p, key, decode_cfg, wm_cfg, sr_cfg)
        wm_att = attack_fn(wm_text, i)
        pl_att = attack_fn(pl_text, i + 10_000)
        wm_scores.append(score_text(wm_att, key, wm_cfg, sr_cfg))
        nu_scores.append(score_text(pl_att, key, wm_cfg, sr_cfg))

    wm_scores = np.array(wm_scores, dtype=np.float64)
    nu_scores = np.array(nu_scores, dtype=np.float64)
    thr = calibrate_threshold(nu_scores, target_fpr=target_fpr)

    return {
        "AUC": auc_from_scores(wm_scores, nu_scores),
        "TPR@1%FPR": float((wm_scores >= thr).mean()),
        "median_wm": float(np.median(wm_scores)),
        "median_null": float(np.median(nu_scores)),
    }

ATTACKS = [
    ("clean", lambda text, seed: text),
    ("syn0.5", lambda text, seed: synonym_replace(text, rate=0.5, seed=GLOBAL_SEED + seed)),
    ("paraphrase_beam5", lambda text, seed: paraphrase_t5(text, seed=GLOBAL_SEED + seed, num_beams=5, do_sample=False)),
]

VARIANTS = [
    ("GPW (no salt)", gpw_nosalt, sr_off),
    ("GPW-SP", gpw_sp, sr_off),
    ("GPW-SP+SR", gpw_sp, sr_on),
]

rows = []
#for vname, vcfg, vsr in VARIANTS:
#    for aname, afn in ATTACKS:
#        res = eval_variant_under_attack(PROMPTS, KEY, decode_cfg, vcfg, vsr, aname, afn, target_fpr=0.01, max_items=20)
#        rows.append({"variant": vname, "attack": aname, **res})

#pilot_df = pd.DataFrame(rows)
#pilot_df.to_csv("paper_results/pilot_variant_table.csv", index=False)
#pilot_df

# --- Pilot variant table after SR fix ---
ATTACKS = [
    ("clean", lambda text, seed: text),
    ("syn0.5", lambda text, seed: synonym_replace(text, rate=0.5, seed=GLOBAL_SEED + seed)),
    ("paraphrase_beam5", lambda text, seed: paraphrase_t5(text, seed=GLOBAL_SEED + seed, num_beams=5, do_sample=False)),
]

VARIANTS = [
    ("GPW (no salt)", gpw_nosalt, sr_off),
    ("GPW-SP", gpw_sp, sr_off),
    ("GPW-SP+SR", gpw_sp, sr_on),
]

def eval_variant_under_attack(prompts, key, decode_cfg, wm_cfg, sr_cfg, attack_name, attack_fn, target_fpr=0.01, max_items=20):
    wm_scores, nu_scores = [], []
    use_prompts = prompts[:max_items]
    for i, p in enumerate(tqdm(use_prompts, desc=f"{attack_name} | {wm_cfg.alpha=} {wm_cfg.omega=} {sr_cfg.enabled=}")):
        wm_text, pl_text = generate_pair(p, key, decode_cfg, wm_cfg, sr_cfg)
        wm_att = attack_fn(wm_text, i)
        pl_att = attack_fn(pl_text, i + 10_000)
        wm_scores.append(score_text(wm_att, key, wm_cfg, sr_cfg))
        nu_scores.append(score_text(pl_att, key, wm_cfg, sr_cfg))

    wm_scores = np.array(wm_scores, dtype=np.float64)
    nu_scores = np.array(nu_scores, dtype=np.float64)
    thr = calibrate_threshold(nu_scores, target_fpr=target_fpr)

    return {
        "AUC": auc_from_scores(wm_scores, nu_scores),
        "TPR@1%FPR": float((wm_scores >= thr).mean()),
        "median_wm": float(np.median(wm_scores)),
        "median_null": float(np.median(nu_scores)),
        "thr": float(thr),
        "n": len(use_prompts),
    }

rows = []
MAX_ITEMS = 40  # bump if you can; 40 is usually stable enough for starter

#for vname, vcfg, vsr in VARIANTS:
    #for aname, afn in ATTACKS:
        #res = eval_variant_under_attack(PROMPTS, KEY, decode_cfg, vcfg, vsr, aname, afn, target_fpr=0.01, max_items=MAX_ITEMS)
        #rows.append({"variant": vname, "attack": aname, **res})

#pilot_df = pd.DataFrame(rows)
#pilot_df.to_csv("paper_results/pilot_variant_table_SRfixed.csv", index=False)
#pilot_df

def run_attack_curve_syn(prompts, key, decode_cfg, wm_cfg, sr_cfg, rates, target_fpr=0.01, max_items=40):
    rows = []
    use_prompts = prompts[:max_items]
    for r in rates:
        wm_scores, nu_scores = [], []
        for i, p in enumerate(tqdm(use_prompts, desc=f"syn_rate={r} | SR={sr_cfg.enabled}")):
            wm_text, pl_text = generate_pair(p, key, decode_cfg, wm_cfg, sr_cfg)
            wm_att = synonym_replace(wm_text, rate=r, seed=GLOBAL_SEED + i)
            pl_att = synonym_replace(pl_text, rate=r, seed=GLOBAL_SEED + 10_000 + i)
            wm_scores.append(score_text(wm_att, key, wm_cfg, sr_cfg))
            nu_scores.append(score_text(pl_att, key, wm_cfg, sr_cfg))

        wm_scores = np.array(wm_scores, dtype=np.float64)
        nu_scores = np.array(nu_scores, dtype=np.float64)
        thr = calibrate_threshold(nu_scores, target_fpr=target_fpr)

        rows.append({
            "variant": "GPW-SP+SR" if sr_cfg.enabled else "GPW-SP",
            "attack": "synonym_replace",
            "strength": r,
            "AUC": auc_from_scores(wm_scores, nu_scores),
            f"TPR@{int(target_fpr*100)}%FPR": float((wm_scores >= thr).mean()),
            "thr": float(thr),
            "median_wm": float(np.median(wm_scores)),
            "median_null": float(np.median(nu_scores)),
            "n": len(use_prompts),
        })
    return pd.DataFrame(rows)

rates = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]

syn_curve_sp  = run_attack_curve_syn(PROMPTS, KEY, decode_cfg, gpw_sp, sr_off, rates, target_fpr=0.01, max_items=40)
syn_curve_srs = run_attack_curve_syn(PROMPTS, KEY, decode_cfg, gpw_sp, sr_on,  rates, target_fpr=0.01, max_items=40)

syn_both = pd.concat([syn_curve_sp, syn_curve_srs], ignore_index=True)
syn_both.to_csv("paper_results/robustness_curve_synonym_SP_vs_SR.csv", index=False)
syn_both

def run_paraphrase_strengths(prompts, key, decode_cfg, wm_cfg, sr_cfg, strengths, target_fpr=0.01, max_items=20):
    rows = []
    use_prompts = prompts[:max_items]
    for (name, kwargs) in strengths:
        wm_scores, nu_scores = [], []
        for i, p in enumerate(tqdm(use_prompts, desc=f"paraphrase={name} | SR={sr_cfg.enabled}")):
            wm_text, pl_text = generate_pair(p, key, decode_cfg, wm_cfg, sr_cfg)
            wm_att = paraphrase_t5(wm_text, seed=GLOBAL_SEED + i, **kwargs)
            pl_att = paraphrase_t5(pl_text, seed=GLOBAL_SEED + 10_000 + i, **kwargs)
            wm_scores.append(score_text(wm_att, key, wm_cfg, sr_cfg))
            nu_scores.append(score_text(pl_att, key, wm_cfg, sr_cfg))

        wm_scores = np.array(wm_scores, dtype=np.float64)
        nu_scores = np.array(nu_scores, dtype=np.float64)
        thr = calibrate_threshold(nu_scores, target_fpr=target_fpr)

        rows.append({
            "variant": "GPW-SP+SR" if sr_cfg.enabled else "GPW-SP",
            "attack": "paraphrase_t5",
            "strength": name,
            "AUC": auc_from_scores(wm_scores, nu_scores),
            f"TPR@{int(target_fpr*100)}%FPR": float((wm_scores >= thr).mean()),
            "thr": float(thr),
            "median_wm": float(np.median(wm_scores)),
            "median_null": float(np.median(nu_scores)),
            "n": len(use_prompts),
        })
    return pd.DataFrame(rows)

strengths = [
    ("beam5", {"num_beams": 5, "do_sample": False}),
    ("beam1", {"num_beams": 1, "do_sample": False}),
]

para_sp  = run_paraphrase_strengths(PROMPTS, KEY, decode_cfg, gpw_sp, sr_off, strengths, target_fpr=0.01, max_items=30)
para_srs = run_paraphrase_strengths(PROMPTS, KEY, decode_cfg, gpw_sp, sr_on,  strengths, target_fpr=0.01, max_items=30)

para_both = pd.concat([para_sp, para_srs], ignore_index=True)
para_both.to_csv("paper_results/robustness_paraphrase_SP_vs_SR.csv", index=False)
para_both

def run_length_scaling(prompts, key, base_decode_cfg, gpw_cfg, sr_cfg, lengths, target_fpr=0.01, max_items=60):
    rows = []
    use_prompts = prompts[:max_items]
    for L in lengths:
        dc = DecodeConfig(
            temperature=base_decode_cfg.temperature,
            top_k=base_decode_cfg.top_k,
            top_p=base_decode_cfg.top_p,
            max_new_tokens=L
        )
        res = run_clean_eval(use_prompts, key, dc, gpw_cfg, sr_cfg, target_fpr=target_fpr, desc=f"Length L={L}")
        rows.append({"L": L, **res})
    return pd.DataFrame(rows)

lengths = [40, 80, 120, 200]
len_df = run_length_scaling(PROMPTS, KEY, decode_cfg, gpw_sp, sr_off, lengths, target_fpr=0.01, max_items=60)
len_df.to_csv("paper_results/length_scaling_SP.csv", index=False)
len_df

def truncate_keep_prefix(text: str, keep_ratio: float) -> str:
    toks = text.split()
    if len(toks) <= 5:
        return text
    k = max(1, int(round(len(toks) * keep_ratio)))
    return " ".join(toks[:k])

def run_trunc_curve(prompts, key, decode_cfg, wm_cfg, sr_cfg, keep_ratios, target_fpr=0.01, max_items=40):
    rows = []
    use_prompts = prompts[:max_items]
    for kr in keep_ratios:
        wm_scores, nu_scores = [], []
        for i, p in enumerate(tqdm(use_prompts, desc=f"truncate_keep={kr}")):
            wm_text, pl_text = generate_pair(p, key, decode_cfg, wm_cfg, sr_cfg)
            wm_att = truncate_keep_prefix(wm_text, keep_ratio=kr)
            pl_att = truncate_keep_prefix(pl_text, keep_ratio=kr)
            wm_scores.append(score_text(wm_att, key, wm_cfg, sr_cfg))
            nu_scores.append(score_text(pl_att, key, wm_cfg, sr_cfg))

        wm_scores = np.array(wm_scores, dtype=np.float64)
        nu_scores = np.array(nu_scores, dtype=np.float64)
        thr = calibrate_threshold(nu_scores, target_fpr=target_fpr)

        rows.append({
            "attack": "truncate_prefix",
            "strength": kr,
            "AUC": auc_from_scores(wm_scores, nu_scores),
            f"TPR@{int(target_fpr*100)}%FPR": float((wm_scores >= thr).mean()),
            "thr": float(thr),
            "median_wm": float(np.median(wm_scores)),
            "median_null": float(np.median(nu_scores)),
            "n": len(use_prompts),
        })
    return pd.DataFrame(rows)

keep_ratios = [1.0, 0.75, 0.5, 0.25]
trunc_df = run_trunc_curve(PROMPTS, KEY, decode_cfg, gpw_sp, sr_off, keep_ratios, target_fpr=0.01, max_items=40)
trunc_df.to_csv("paper_results/robustness_truncation_curve_SP.csv", index=False)
trunc_df

# --- EXP 6: SR sweep ---
LAMBDAS = [0.0, 0.05, 0.1]
RANKS   = [4, 8]

ATTACKS = [
    ("paraphrase_beam5", lambda text, seed: paraphrase_t5(text, seed=GLOBAL_SEED + seed, num_beams=5, do_sample=False)),
    ("clean", lambda text, seed: text),
    ("syn0.5", lambda text, seed: synonym_replace(text, rate=0.5, seed=GLOBAL_SEED + seed)),
]

MAX_ITEMS = 15  # set 40 if you can; start with 30 to keep sweep manageable

rows = []
for lam in LAMBDAS:
    for r in RANKS:
        sr_cfg = SRConfig(enabled=(lam > 0), lambda_couple=lam, rank=r)
        for aname, afn in ATTACKS:
            res = eval_variant_under_attack(
                PROMPTS, KEY, decode_cfg,
                wm_cfg=gpw_sp, sr_cfg=sr_cfg,
                attack_name=aname, attack_fn=afn,
                target_fpr=0.01, max_items=MAX_ITEMS
            )
            rows.append({
                "lambda": lam,
                "rank": r,
                "attack": aname,
                **res
            })

sr_sweep_df = pd.DataFrame(rows).sort_values(["attack", "AUC"], ascending=[True, False])
sr_sweep_df.to_csv("paper_results/sr_lambda_rank_sweep.csv", index=False)
sr_sweep_df

# --- EXP 7: decoding sensitivity ---
DECODE_GRID = [
    ("base",   DecodeConfig(temperature=0.9, top_k=50, top_p=0.95, max_new_tokens=120)),
    ("temp07", DecodeConfig(temperature=0.7, top_k=50, top_p=0.95, max_new_tokens=120)),
    ("temp11", DecodeConfig(temperature=1.1, top_k=50, top_p=0.95, max_new_tokens=120)),
    ("tp90",   DecodeConfig(temperature=0.9, top_k=50, top_p=0.90, max_new_tokens=120)),
    ("tp100",  DecodeConfig(temperature=0.9, top_k=50, top_p=1.00, max_new_tokens=120)),
    ("k0",     DecodeConfig(temperature=0.9, top_k=0,  top_p=0.95, max_new_tokens=120)),  # no top-k
]

MAX_ITEMS = 15

rows = []
for name, dc in DECODE_GRID:
    res = run_clean_eval(PROMPTS[:MAX_ITEMS], KEY, dc, gpw_sp, sr_off, target_fpr=0.01, desc=f"Decode={name}")
    rows.append({"decode": name, **res, "temp": dc.temperature, "top_k": dc.top_k, "top_p": dc.top_p, "L": dc.max_new_tokens})

decode_df = pd.DataFrame(rows)
decode_df.to_csv("paper_results/decoding_sensitivity.csv", index=False)
decode_df

def word_delete(text: str, ratio: float, seed: int = 0):
    rng = random.Random(seed)
    toks = text.split()
    if len(toks) <= 6:
        return text
    keep = [w for w in toks if rng.random() > ratio]
    return " ".join(keep) if keep else toks[0]

def run_delete_curve(ratios, max_items=40):
    rows = []
    for r in ratios:
        row, _ = run_attack_eval(
            prompts=PROMPTS[:max_items],
            attack_name=f"delete_{r}",
            attack_fn=lambda txt, seed, rr=r: word_delete(txt, rr, seed=GLOBAL_SEED+seed),
            gpw_cfg=gpw_sp,
            sr_cfg=sr_off,
            decode_cfg=decode_cfg,
            target_fpr=0.01,
            max_items=max_items,
        )
        rows.append(row)
    return pd.DataFrame(rows)

del_df = run_delete_curve([0.0, 0.2, 0.4], max_items=20)
del_df.to_csv("paper_results/robustness_delete_curve.csv", index=False)
del_df

def mix_concat(wm_text: str, plain_text: str, wm_fraction: float) -> str:
    # Simple token-level mixing by words: prefix from one + suffix from other
    wm = wm_text.split()
    pl = plain_text.split()
    if len(wm) < 8 or len(pl) < 8:
        return wm_text
    k = int(round(wm_fraction * len(wm)))
    k = max(1, min(k, len(wm)-1))
    # wm prefix + plain suffix
    return " ".join(wm[:k] + pl[k:])

def run_mix_curve(fractions, max_items=40):
    rows = []
    use_prompts = PROMPTS[:max_items]
    for f in fractions:
        wm_scores, nu_scores = [], []
        for i, p in enumerate(tqdm(use_prompts, desc=f"mix_wm_fraction={f}")):
            wm_text, pl_text = generate_pair(p, KEY, decode_cfg, gpw_sp, sr_off)
            mixed_wm = mix_concat(wm_text, pl_text, wm_fraction=f)
            mixed_pl = mix_concat(pl_text, wm_text, wm_fraction=f)  # null control
            wm_scores.append(score_text(mixed_wm, KEY, gpw_sp, sr_off))
            nu_scores.append(score_text(mixed_pl, KEY, gpw_sp, sr_off))

        wm_scores = np.array(wm_scores, dtype=np.float64)
        nu_scores = np.array(nu_scores, dtype=np.float64)
        thr = calibrate_threshold(nu_scores, target_fpr=0.01)

        rows.append({
            "attack": "mix_concat",
            "strength": f,
            "AUC": auc_from_scores(wm_scores, nu_scores),
            "TPR@1%FPR": float((wm_scores >= thr).mean()),
            "thr": float(thr),
            "median_wm": float(np.median(wm_scores)),
            "median_null": float(np.median(nu_scores)),
            "n": len(use_prompts),
        })
    return pd.DataFrame(rows)

mix_df = run_mix_curve([1.0, 0.75, 0.5, 0.25], max_items=20)
mix_df.to_csv("paper_results/robustness_mix_curve.csv", index=False)
mix_df

# --- EXP 9: cross-domain calibration ---
# Calibrate on C4 plain, test FPR on TruthfulQA plain (or vice versa)

def load_prompts_for_domain(domain: str, n=80, seed=1234):
    if domain == "truthfulqa":
        return load_prompts_truthfulqa(n=n, seed=seed)
    if domain == "c4":
        return load_prompts_c4(n=n, seed=seed, max_chars=MAX_CHARS)
    raise ValueError(domain)

N_CAL = 80
N_TEST = 80

cal_prompts = load_prompts_for_domain("c4", n=N_CAL, seed=GLOBAL_SEED)
test_prompts = load_prompts_for_domain("truthfulqa", n=N_TEST, seed=GLOBAL_SEED+1)

# Generate plain texts for calibration + testing
plain_cfg = GPWConfig(alpha=0.0, omega=gpw_sp.omega, salted=gpw_sp.salted, ctx_mode=gpw_sp.ctx_mode, ngram=gpw_sp.ngram)

def gen_plain_scores(prompts, nmax=80):
    scores = []
    for p in tqdm(prompts[:nmax], desc="gen plain"):
        pl_text, _ = gpw_generate(p, KEY, decode_cfg, plain_cfg, sr_off, PayloadConfig(False), return_debug=False)
        scores.append(score_text(pl_text, KEY, gpw_sp, sr_off))
    return np.array(scores, dtype=np.float64)

cal_scores = gen_plain_scores(cal_prompts, nmax=N_CAL)
thr = calibrate_threshold(cal_scores, target_fpr=0.01)

test_scores = gen_plain_scores(test_prompts, nmax=N_TEST)
emp_fpr = float((test_scores >= thr).mean())

{"thr_calibrated_on_c4": float(thr), "empirical_FPR_on_truthfulqa": emp_fpr, "n_test": len(test_scores)}

"""---
---
---
---

## INIT
"""

# -*- coding: utf-8 -*-
"""GPW_SP_Contextual_Cluster_WM.py

Cleaned and structured version of the Gaussian Pancakes Watermarking script.
"""

######
# Initializer Pip Installs
######
!pip -q install transformers accelerate sentencepiece datasets evaluate scikit-learn nltk

######
# Basic Imports
######
import os
import math
import hashlib
import random
import time
import json
import glob
import re
from dataclasses import dataclass
from typing import Optional, Dict, Any, List, Tuple
from datetime import datetime

import numpy as np
import pandas as pd
from tqdm import tqdm

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM

from sklearn.metrics import roc_auc_score, roc_curve

import nltk
from nltk.corpus import wordnet as wn

# Ensure NLTK data is available
try:
    wn.ensure_loaded()
except Exception:
    nltk.download("wordnet")
    nltk.download("omw-1.4")

######
# System Setup
######
GLOBAL_SEED = 1234
random.seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)
torch.manual_seed(GLOBAL_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(GLOBAL_SEED)

device = "cuda" if torch.cuda.is_available() else "cpu"

MODEL_NAME = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)
model.eval()

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

######
# Configurations
######
@dataclass
class DecodeConfig:
    temperature: float = 1.0
    top_k: int = 0         # 0 disables top-k
    top_p: float = 1.0     # 1.0 disables nucleus
    max_new_tokens: int = 128

@dataclass
class GPWConfig:
    alpha: float = 1.0
    omega: float = 10.0
    salted: bool = True
    ctx_mode: str = "prev_token"   # "prev_token" | "ngram" | "rolling"
    ngram: int = 4                 # used if ctx_mode == "ngram"

@dataclass
class SRConfig:
    enabled: bool = False
    lambda_couple: float = 0.2
    rank: int = 8

@dataclass
class PayloadConfig:
    enabled: bool = False
    segment_len: int = 32

######
# Crypto Utilities
######
def _hmac_like(key: bytes, msg: bytes) -> bytes:
    # Lightweight "PRF-like" keyed hash.
    return hashlib.sha256(key + b"||" + msg).digest()

def prf_to_uniform01(key: bytes, msg: bytes) -> float:
    # Map PRF output to [0,1)
    digest = _hmac_like(key, msg)
    x = int.from_bytes(digest[:8], "big")  # 64-bit
    return (x % (2**53)) / float(2**53)

def seeded_torch_generator(key: bytes, tag: bytes, device: str) -> torch.Generator:
    # Make a torch.Generator whose device matches the tensors you'll sample.
    seed_bytes = _hmac_like(key, tag)
    seed = int.from_bytes(seed_bytes[:8], "big") % (2**63 - 1)
    g = torch.Generator(device=device)
    g.manual_seed(seed)
    return g

######
# Projections and Projections Helpers
######
@torch.no_grad()
def get_token_embedding_matrix(model) -> torch.Tensor:
    return model.get_input_embeddings().weight

@torch.no_grad()
def derive_secret_direction_w(key: bytes, d: int, device: str) -> torch.Tensor:
    g = seeded_torch_generator(key, b"GPW_w", device=device)
    v = torch.randn(d, generator=g, device=device, dtype=torch.float32)
    v = v / (v.norm() + 1e-12)
    return v

@torch.no_grad()
def precompute_projections(E: torch.Tensor, w: torch.Tensor) -> torch.Tensor:
    return (E @ w).float()

######
# Context Fingerprinting and Phase Logic
######
def ctx_fingerprint(input_ids: torch.Tensor, mode: str, ngram: int = 4) -> bytes:
    ids = input_ids[0].tolist()
    if len(ids) == 0:
        return b"empty"

    if mode == "prev_token":
        return f"prev:{ids[-1]}".encode()

    if mode == "ngram":
        tail = ids[-ngram:] if len(ids) >= ngram else ids
        return ("ngram:" + ",".join(map(str, tail))).encode()

    if mode == "rolling":
        h = hashlib.sha256(("roll:" + ",".join(map(str, ids[-64:]))).encode()).digest()
        return b"roll:" + h[:16]

    raise ValueError(f"Unknown ctx_mode={mode}")

def salted_phase_phi(key: bytes, input_ids: torch.Tensor, cfg: GPWConfig) -> float:
    fp = ctx_fingerprint(input_ids, cfg.ctx_mode, cfg.ngram)
    u = prf_to_uniform01(key, b"phi||" + fp)
    return 2.0 * math.pi * u

######
# Semantic Representation (SR) Helpers
######
@torch.no_grad()
def make_low_rank_A(key: bytes, d_embed: int, d_hid: int, rank: int, device: str) -> torch.Tensor:
    gB = seeded_torch_generator(key, b"SR_B", device=device)
    gC = seeded_torch_generator(key, b"SR_C", device=device)
    B = torch.randn(d_embed, rank, generator=gB, device=device, dtype=torch.float32) / math.sqrt(d_embed)
    C = torch.randn(rank, d_hid, generator=gC, device=device, dtype=torch.float32) / math.sqrt(d_hid)
    return B @ C

@torch.no_grad()
def make_low_rank_A_factors(key: bytes, d_embed: int, d_hid: int, rank: int, device: str):
    gB = seeded_torch_generator(key, b"SR_B", device=device)
    gC = seeded_torch_generator(key, b"SR_C", device=device)
    B = torch.randn(d_embed, rank, generator=gB, device=device, dtype=torch.float32) / math.sqrt(d_embed)
    C = torch.randn(rank, d_hid, generator=gC, device=device, dtype=torch.float32) / math.sqrt(d_hid)
    return B, C

@torch.no_grad()
def compute_w_t(w: torch.Tensor, A: torch.Tensor, h_t: torch.Tensor, lambda_couple: float) -> torch.Tensor:
    v = w + lambda_couple * (A @ h_t)
    return v / (v.norm() + 1e-12)

######
# Scoring and Filtering
######
@torch.no_grad()
def pancake_score(s: torch.Tensor, omega: float, phi: float) -> torch.Tensor:
    return torch.cos(omega * s + phi)

def top_k_filter(logits: torch.Tensor, k: int) -> torch.Tensor:
    if k <= 0 or k >= logits.size(-1):
        return logits
    values, _ = torch.topk(logits, k)
    thresh = values[..., -1, None]
    return torch.where(logits < thresh, torch.full_like(logits, -1e10), logits)

def top_p_filter(logits: torch.Tensor, p: float) -> torch.Tensor:
    if p >= 1.0:
        return logits
    sorted_logits, sorted_idx = torch.sort(logits, descending=True, dim=-1)
    probs = F.softmax(sorted_logits, dim=-1)
    cum = torch.cumsum(probs, dim=-1)
    mask = cum > p
    mask[..., 0] = False
    sorted_logits = sorted_logits.masked_fill(mask, -1e10)
    out = torch.full_like(logits, -1e10)
    out.scatter_(dim=-1, index=sorted_idx, src=sorted_logits)
    return out

def sample_from_logits(logits: torch.Tensor, temperature: float, top_k: int, top_p: float, key: bytes, step_tag: bytes) -> int:
    logits = logits / max(temperature, 1e-6)
    logits = top_k_filter(logits, top_k)
    logits = top_p_filter(logits, top_p)
    probs = F.softmax(logits, dim=-1)
    g = seeded_torch_generator(key, b"SAMPLE||" + step_tag, device=str(probs.device))
    idx = torch.multinomial(probs, num_samples=1, generator=g)
    return int(idx.item())

######
# Main Watermarking Generation
######
@torch.no_grad()
def gpw_generate(
    prompt: str,
    key: bytes,
    decode_cfg: DecodeConfig,
    gpw_cfg: GPWConfig,
    sr_cfg: SRConfig,
    payload_cfg: PayloadConfig,
    return_debug: bool = False
) -> Tuple[str, Optional[Dict[str, Any]]]:

    enc = tokenizer(prompt, return_tensors="pt").to(device)
    input_ids = enc["input_ids"]

    E = get_token_embedding_matrix(model).to(device)
    V, d_embed = E.shape

    w = derive_secret_direction_w(key, d_embed, device=device)
    s_base = precompute_projections(E, w)

    A = None
    if sr_cfg.enabled:
        d_hid = model.config.n_embd if hasattr(model.config, "n_embd") else model.config.hidden_size
        A = make_low_rank_A(key, d_embed, d_hid, sr_cfg.rank, device=device)

    debug = {"scores": [], "phis": [], "tokens": []} if return_debug else None

    for t in range(decode_cfg.max_new_tokens):
        outputs = model(input_ids=input_ids, output_hidden_states=sr_cfg.enabled)
        logits = outputs.logits[0, -1, :]

        phi = 0.0
        if gpw_cfg.salted:
            phi = salted_phase_phi(key, input_ids, gpw_cfg)

        if sr_cfg.enabled:
            h_t = outputs.hidden_states[-1][0, -1, :].float()
            w_t = compute_w_t(w, A, h_t, sr_cfg.lambda_couple)
            s = precompute_projections(E, w_t)
        else:
            s = s_base

        g = pancake_score(s, gpw_cfg.omega, phi)
        logits_wm = logits + gpw_cfg.alpha * g

        next_id = sample_from_logits(
            logits_wm,
            temperature=decode_cfg.temperature,
            top_k=decode_cfg.top_k,
            top_p=decode_cfg.top_p,
            key=key,
            step_tag=f"t={t}".encode(),
        )

        if return_debug:
            debug["phis"].append(phi)
            debug["scores"].append(float(g[next_id].detach().cpu()))
            debug["tokens"].append(next_id)

        input_ids = torch.cat([input_ids, torch.tensor([[next_id]], device=device)], dim=1)
        if next_id == tokenizer.eos_token_id:
            break

    text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
    return text, debug

######
# Scoring and Detection Utilities
######
class GPWPrecomp:
    def __init__(self, model, key: bytes, device: str):
        E = model.get_input_embeddings().weight.detach().to(device)
        V, d = E.shape
        w = derive_secret_direction_w(key, d, device=device)
        s_base = (E @ w).float()
        self.E = E
        self.V = V
        self.d = d
        self.w = w
        self.s_base = s_base

@torch.no_grad()
def gpw_score_text_fast(text: str, key: bytes, gpw_cfg: GPWConfig, precomp: GPWPrecomp) -> float:
    enc = tokenizer(text, return_tensors="pt", truncation=True).input_ids[0].tolist()
    if len(enc) <= 1:
        return 0.0

    S = 0.0
    for t in range(1, len(enc)):
        token = enc[t]
        prefix = enc[:t]
        fp = _ctx_fp_from_ids(prefix, gpw_cfg.ctx_mode, gpw_cfg.ngram)
        phi = 2.0 * math.pi * prf_to_uniform01(key, b"phi||" + fp) if gpw_cfg.salted else 0.0
        s_tok = float(precomp.s_base[token].detach().cpu())
        S += math.cos(gpw_cfg.omega * s_tok + phi)
    return float(S)

def _ctx_fp_from_ids(ids: List[int], mode: str, ngram: int) -> bytes:
    if len(ids) == 0:
        return b"empty"
    if mode == "prev_token":
        return f"prev:{ids[-1]}".encode()
    if mode == "ngram":
        tail = ids[-ngram:] if len(ids) >= ngram else ids
        return ("ngram:" + ",".join(map(str, tail))).encode()
    if mode == "rolling":
        h = hashlib.sha256(("roll:" + ",".join(map(str, ids[-64:]))).encode()).digest()
        return b"roll:" + h[:16]
    raise ValueError(f"Unknown ctx_mode={mode}")

@torch.no_grad()
def gpw_score_text_sr_exact(text: str, key: bytes, gpw_cfg: GPWConfig, sr_cfg: SRConfig, precomp: GPWPrecomp) -> float:
    enc = tokenizer(text, return_tensors="pt", truncation=True).to(device)
    input_ids = enc["input_ids"]
    T = input_ids.size(1)
    if T <= 1:
        return 0.0

    out = model(input_ids=input_ids, output_hidden_states=True)
    h = out.hidden_states[-1][0].float()

    d_hid = h.size(-1)
    B, C = make_low_rank_A_factors(key, precomp.d, d_hid, sr_cfg.rank, device=device)

    ids = input_ids[0].tolist()
    S = 0.0
    for t in range(1, T):
        token = ids[t]
        prefix_ids = ids[:t]
        fp = _ctx_fp_from_ids(prefix_ids, gpw_cfg.ctx_mode, gpw_cfg.ngram)
        phi = 2.0 * math.pi * prf_to_uniform01(key, b"phi||" + fp) if gpw_cfg.salted else 0.0
        u = (C @ h[t-1]).float()
        delta = (B @ u).float()
        v = (precomp.w + sr_cfg.lambda_couple * delta).float()
        v = v / (v.norm() + 1e-12)
        e_tok = precomp.E[token].float()
        s_tok = float((e_tok @ v).detach().cpu())
        S += math.cos(gpw_cfg.omega * s_tok + phi)
    return float(S)

def random_key_p_value(text: str, true_key: bytes, gpw_cfg: GPWConfig, sr_cfg: SRConfig, trials: int = 200) -> float:
    # Uses gpw_score_text_exact based on presence of SR
    precomp_true = GPWPrecomp(model, true_key, device)
    if not sr_cfg.enabled:
        obs = gpw_score_text_fast(text, true_key, gpw_cfg, precomp_true)
    else:
        obs = gpw_score_text_sr_exact(text, true_key, gpw_cfg, sr_cfg, precomp_true)

    better = 0
    for i in range(trials):
        fake_key = hashlib.sha256(b"fake||" + i.to_bytes(4,"big") + true_key).digest()
        precomp_fake = GPWPrecomp(model, fake_key, device)
        if not sr_cfg.enabled:
            s = gpw_score_text_fast(text, fake_key, gpw_cfg, precomp_fake)
        else:
            s = gpw_score_text_sr_exact(text, fake_key, gpw_cfg, sr_cfg, precomp_fake)
        if s >= obs:
            better += 1
    return (better + 1) / (trials + 1)

######
# Data and Prompt Loading
######
from datasets import load_dataset

def load_prompts_truthfulqa(n=80, seed=1234):
    ds = load_dataset("truthful_qa", "generation", split="validation")
    qs = list(ds["question"])
    rng = random.Random(seed)
    rng.shuffle(qs)
    return qs[:n]

def load_prompts_c4(n=80, seed=1234, max_chars=240):
    ds = load_dataset("c4", "en", split=f"train[:{max(5*n, n)}]")
    texts = []
    for ex in ds:
        txt = ex["text"].strip().replace("\n", " ")
        if txt:
            texts.append(txt[:max_chars])
        if len(texts) >= n:
            break
    rng = random.Random(seed)
    rng.shuffle(texts)
    return texts[:n]

######
# Attack Functions
######
_WORD_RE = re.compile(r"\b[\w']+\b")
STOPWORDS = set("a an the and or but if then else for with without to of in on at by from as is are was were be been being this that these those it its i you he she they we my your our their".split())

def _get_wordnet_synonym(word: str):
    w = word.lower()
    syns = set()
    for s in wn.synsets(w):
        for lemma in s.lemmas():
            cand = lemma.name().replace("_", " ")
            if cand.lower() != w and cand.isalpha():
                syns.add(cand)
    if not syns: return None
    return sorted(syns, key=len)[0]

def synonym_replace(text: str, rate: float, seed: int = 0) -> str:
    rng = random.Random(seed)
    matches = list(_WORD_RE.finditer(text))
    if not matches: return text
    eligible = []
    for mi, m in enumerate(matches):
        w = m.group(0)
        if w.lower() in STOPWORDS or not w.isalpha(): continue
        syn = _get_wordnet_synonym(w)
        if syn: eligible.append((mi, syn))
    if not eligible or rate <= 0: return text
    k = min(len(eligible), max(1, int(round(rate * len(eligible)))))
    chosen = rng.sample(eligible, k=k)
    rep = {mi: syn for (mi, syn) in chosen}
    out, last = [], 0
    for mi, m in enumerate(matches):
        out.append(text[last:m.start()])
        w = m.group(0)
        if mi in rep:
            syn = rep[mi]
            if w[0].isupper(): syn = syn.capitalize()
            out.append(syn)
        else: out.append(w)
        last = m.end()
    out.append(text[last:])
    return "".join(out)

######
# Paraphrase Attack Tools
######
PARA_MODEL = "ramsrigouthamg/t5_paraphraser"
para_tok = AutoTokenizer.from_pretrained(PARA_MODEL)
para_model = AutoModelForSeq2SeqLM.from_pretrained(PARA_MODEL).to(device)
para_model.eval()

@torch.no_grad()
def paraphrase_t5(text: str, seed: int = 0, num_beams: int = 5, do_sample: bool = False, max_new_tokens: int = 128) -> str:
    torch.manual_seed(seed)
    inp = "paraphrase: " + text + " </s>"
    enc = para_tok([inp], return_tensors="pt", truncation=True).to(device)
    out_ids = para_model.generate(**enc, num_beams=num_beams, do_sample=do_sample, max_new_tokens=max_new_tokens, early_stopping=True)
    return para_tok.decode(out_ids[0], skip_special_tokens=True).strip() or text

######
# Global Initializers
######
KEY = hashlib.sha256(b"gpw-standard-key").digest()
PROMPTS = load_prompts_truthfulqa(n=50, seed=GLOBAL_SEED)

decode_cfg_default = DecodeConfig(temperature=0.9, top_k=50, top_p=0.95, max_new_tokens=120)
gpw_cfg_standard = GPWConfig(alpha=1.2, omega=10.0, salted=True, ctx_mode="prev_token")
sr_off = SRConfig(enabled=False)
sr_on = SRConfig(enabled=True, lambda_couple=0.2, rank=8)

if __name__ == "__main__":
    print(f"Modules initialized. Device: {device}")
    print(f"Sample prompt: {PROMPTS[0]}")

def make_my_variant(alpha, omega):
    # example: salted + SR with different (lambda, rank)
    return GPWConfig(alpha=alpha, omega=omega, salted=True, ctx_mode="prev_token", ngram=4), SRConfig(enabled=True, lambda_couple=0.10, rank=4)

METHODS.append({"name": "GPW-SP+SR (lam=0.10,r=4)", "make_cfg": lambda a, w: make_my_variant(a, w)})

# @title  Run Experiments & Visualize Pareto Frontier (GPW family + plug-in new methods)
# Run AFTER your initializer cell (the one that defines gpw_generate, GPWConfig, SRConfig, DecodeConfig, etc.)

import math, hashlib, random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.metrics import roc_auc_score

# ---------------------------
# 0) Sanity checks
# ---------------------------
_required = ["gpw_generate", "GPWConfig", "SRConfig", "DecodeConfig", "tokenizer", "model", "device"]
missing = [x for x in _required if x not in globals()]
if missing:
    raise RuntimeError(f"Missing globals {missing}. Run your initializer cell first.")

# ---------------------------
# 1) Utility: generation that also returns prompt length for continuation-PPL
# ---------------------------
@torch.no_grad()
def gpw_generate_ids(
    prompt: str,
    key: bytes,
    decode_cfg: DecodeConfig,
    gpw_cfg: GPWConfig,
    sr_cfg: SRConfig,
):
    enc = tokenizer(prompt, return_tensors="pt").to(device)
    input_ids = enc["input_ids"]
    start_len = input_ids.size(1)

    # copy of gpw_generate but returns ids
    E = model.get_input_embeddings().weight.to(device)
    V, d_embed = E.shape

    w = derive_secret_direction_w(key, d_embed, device=device)
    s_base = precompute_projections(E, w)

    A = None
    if sr_cfg.enabled:
        d_hid = model.config.n_embd if hasattr(model.config, "n_embd") else model.config.hidden_size
        A = make_low_rank_A(key, d_embed, d_hid, sr_cfg.rank, device=device)

    for t in range(decode_cfg.max_new_tokens):
        outputs = model(input_ids=input_ids, output_hidden_states=sr_cfg.enabled)
        logits = outputs.logits[0, -1, :]

        phi = salted_phase_phi(key, input_ids, gpw_cfg) if gpw_cfg.salted else 0.0

        if sr_cfg.enabled:
            h_t = outputs.hidden_states[-1][0, -1, :].float()
            w_t = compute_w_t(w, A, h_t, sr_cfg.lambda_couple)
            s = precompute_projections(E, w_t)
        else:
            s = s_base

        g = pancake_score(s, gpw_cfg.omega, phi)
        logits_wm = logits + gpw_cfg.alpha * g

        next_id = sample_from_logits(
            logits_wm,
            temperature=decode_cfg.temperature,
            top_k=decode_cfg.top_k,
            top_p=decode_cfg.top_p,
            key=key,
            step_tag=f"t={t}".encode(),
        )
        input_ids = torch.cat([input_ids, torch.tensor([[next_id]], device=device)], dim=1)
        if next_id == tokenizer.eos_token_id:
            break

    return input_ids, start_len

@torch.no_grad()
def ppl_on_continuation(input_ids: torch.Tensor, prompt_len: int) -> float:
    """
    Perplexity on generated continuation only (labels masked on the prompt).
    """
    if input_ids.size(1) <= max(1, prompt_len):
        return float("nan")
    labels = input_ids.clone()
    labels[:, :prompt_len] = -100
    out = model(input_ids=input_ids, labels=labels)
    loss = float(out.loss.detach().cpu())
    return float(math.exp(loss))

# ---------------------------
# 2) Detection score wrappers (fast path for SR-off; exact for SR-on)
# ---------------------------
_PRECOMP_CACHE = {}

def get_precomp(key: bytes):
    k = hashlib.sha256(key).hexdigest()
    if k not in _PRECOMP_CACHE:
        _PRECOMP_CACHE[k] = GPWPrecomp(model, key, device)
    return _PRECOMP_CACHE[k]

@torch.no_grad()
def detect_score(text: str, key: bytes, gpw_cfg: GPWConfig, sr_cfg: SRConfig) -> float:
    precomp = get_precomp(key)
    if not sr_cfg.enabled:
        return gpw_score_text_fast(text, key, gpw_cfg, precomp)
    else:
        return gpw_score_text_sr_exact(text, key, gpw_cfg, sr_cfg, precomp)

# ---------------------------
# 3) Attacks for "robustness axis" (cheap + consistent)
# ---------------------------
def attack_one_word_flip(text: str, seed: int = 0) -> str:
    rng = random.Random(seed)
    words = text.split()
    if len(words) < 12:
        return text
    mid = len(words) // 2
    words[mid] = "thing"
    return " ".join(words)

def attack_synonym(text: str, seed: int = 0, rate: float = 0.2) -> str:
    return synonym_replace(text, rate=rate, seed=seed)

# Choose which robustness attack to use in the Pareto run:
ROBUST_ATTACK_NAME = "synonym_0.2"     # "one_word_flip" or "synonym_0.2"
def robust_attack_fn(text: str, seed: int) -> str:
    if ROBUST_ATTACK_NAME == "one_word_flip":
        return attack_one_word_flip(text, seed)
    return attack_synonym(text, seed, rate=0.2)

# ---------------------------
# 4) Core evaluation for one setting
# ---------------------------
def calibrate_threshold(null_scores: np.ndarray, target_fpr: float = 0.01) -> float:
    # higher => more watermarked
    return float(np.quantile(null_scores, 1.0 - target_fpr))

def eval_setting(
    prompts,
    base_key: bytes,
    decode_cfg: DecodeConfig,
    wm_cfg: GPWConfig,
    sr_cfg: SRConfig,
    target_fpr: float = 0.01,
    seed_offset: int = 0,
):
    wm_scores, null_scores = [], []
    wm_scores_rob = []
    ppl_wm_list, ppl_null_list = [], []

    for i, p in enumerate(prompts):
        # deterministically vary run keys to reduce "same randomness path" artifacts
        k_wm = hashlib.sha256(base_key + f"|wm|{i+seed_offset}".encode()).digest()
        k_pl = hashlib.sha256(base_key + f"|pl|{i+seed_offset}".encode()).digest()

        # --- Plain (null) generation: alpha=0 but detect using wm_cfg params ---
        null_cfg = GPWConfig(
            alpha=0.0, omega=wm_cfg.omega, salted=wm_cfg.salted,
            ctx_mode=wm_cfg.ctx_mode, ngram=wm_cfg.ngram
        )
        ids_null, plen_null = gpw_generate_ids(p, k_pl, decode_cfg, null_cfg, sr_cfg)
        text_null = tokenizer.decode(ids_null[0], skip_special_tokens=True)
        s_null = detect_score(text_null, k_pl, wm_cfg, sr_cfg)
        null_scores.append(s_null)
        ppl_null_list.append(ppl_on_continuation(ids_null, plen_null))

        # --- Watermarked generation ---
        ids_wm, plen_wm = gpw_generate_ids(p, k_wm, decode_cfg, wm_cfg, sr_cfg)
        text_wm = tokenizer.decode(ids_wm[0], skip_special_tokens=True)
        s_wm = detect_score(text_wm, k_wm, wm_cfg, sr_cfg)
        wm_scores.append(s_wm)
        ppl_wm_list.append(ppl_on_continuation(ids_wm, plen_wm))

        # --- Robustness: attack then rescore (same key/config) ---
        attacked = robust_attack_fn(text_wm, seed=i+seed_offset)
        s_rob = detect_score(attacked, k_wm, wm_cfg, sr_cfg)
        wm_scores_rob.append(s_rob)

    wm_scores = np.array(wm_scores, dtype=np.float64)
    null_scores = np.array(null_scores, dtype=np.float64)
    wm_scores_rob = np.array(wm_scores_rob, dtype=np.float64)
    ppl_wm = np.array(ppl_wm_list, dtype=np.float64)
    ppl_null = np.array(ppl_null_list, dtype=np.float64)

    thr = calibrate_threshold(null_scores, target_fpr=target_fpr)
    tpr = float((wm_scores >= thr).mean())

    y = np.concatenate([np.ones_like(wm_scores), np.zeros_like(null_scores)])
    s = np.concatenate([wm_scores, null_scores])
    auc = float(roc_auc_score(y, s))

    out = {
        "AUC": auc,
        "TPR@1%FPR": tpr,
        "thr": thr,
        "median_wm": float(np.median(wm_scores)),
        "median_null": float(np.median(null_scores)),
        "median_rob": float(np.median(wm_scores_rob)),
        "ppl_wm": float(np.nanmean(ppl_wm)),
        "ppl_null": float(np.nanmean(ppl_null)),
        "delta_ppl": float(np.nanmean(ppl_wm - ppl_null)),
        "n": int(len(prompts)),
    }
    return out

# ---------------------------
# 5) Define methods (plug in new ones here)
# ---------------------------
# You can add new methods by appending to METHODS with a make_cfg(alpha, omega) function.

def make_gpw_no_salt(alpha, omega):
    return GPWConfig(alpha=alpha, omega=omega, salted=False, ctx_mode="prev_token", ngram=4), SRConfig(enabled=False)

def make_gpw_sp(alpha, omega):
    return GPWConfig(alpha=alpha, omega=omega, salted=True, ctx_mode="prev_token", ngram=4), SRConfig(enabled=False)

def make_gpw_sp_sr(alpha, omega, lam=0.05, rank=8):
    return GPWConfig(alpha=alpha, omega=omega, salted=True, ctx_mode="prev_token", ngram=4), SRConfig(enabled=True, lambda_couple=lam, rank=rank)

METHODS = [
    {"name": "GPW (no salt)", "make_cfg": lambda a, w: make_gpw_no_salt(a, w)},
    {"name": "GPW-SP",        "make_cfg": lambda a, w: make_gpw_sp(a, w)},
    {"name": "GPW-SP+SR",     "make_cfg": lambda a, w: make_gpw_sp_sr(a, w, lam=0.05, rank=8)},
]

# ---------------------------
# 6) Experiment configuration (edit this)
# ---------------------------
BASE_KEY = hashlib.sha256(b"pareto-key").digest()

# Keep this small for budget; bump if you have room.
N_PROMPTS = 12

# Use your dataset loaders if you prefer; otherwise small custom prompts:
try:
    prompts = load_prompts_truthfulqa(n=N_PROMPTS, seed=GLOBAL_SEED)
except Exception:
    prompts = [
        "The scientific method is a systematic way of learning about",
        "In the future, artificial intelligence will likely",
        "The economic impact of climate change is",
        "History shows that civilizations often collapse due to",
    ][:N_PROMPTS]

strengths = [0.5, 1.0, 1.5, 2.0]   # alpha
frequencies = [5.0, 10.0, 25.0]    # omega
decode_cfg = DecodeConfig(temperature=0.9, top_k=50, top_p=0.95, max_new_tokens=60)

TARGET_FPR = 0.01
Y_METRIC = "TPR@1%FPR"  # or "AUC"
X_METRIC = "delta_ppl"  # or "ppl_wm"

print(f"Running Pareto sweep with robustness attack = {ROBUST_ATTACK_NAME}")
print(f"Prompts: {len(prompts)}, grid: {len(METHODS)} methods x {len(strengths)} alphas x {len(frequencies)} omegas")

# ---------------------------
# 7) Run grid
# ---------------------------
rows = []
pbar = tqdm(total=len(METHODS)*len(strengths)*len(frequencies), desc="Grid")

for m in METHODS:
    for a in strengths:
        for w in frequencies:
            wm_cfg, sr_cfg = m["make_cfg"](a, w)
            res = eval_setting(prompts, BASE_KEY, decode_cfg, wm_cfg, sr_cfg, target_fpr=TARGET_FPR)
            res.update({
                "method": m["name"],
                "alpha": a,
                "omega": w,
                "salted": wm_cfg.salted,
                "SR": sr_cfg.enabled,
                "lambda": getattr(sr_cfg, "lambda_couple", 0.0),
                "rank": getattr(sr_cfg, "rank", 0),
                "attack": ROBUST_ATTACK_NAME,
                "decode_temp": decode_cfg.temperature,
                "decode_top_k": decode_cfg.top_k,
                "decode_top_p": decode_cfg.top_p,
                "max_new_tokens": decode_cfg.max_new_tokens,
            })
            rows.append(res)
            pbar.update(1)

pbar.close()
df = pd.DataFrame(rows)
display(df.sort_values([Y_METRIC, X_METRIC], ascending=[False, True]).head(10))

# ---------------------------
# 8) Pareto frontier helper
# ---------------------------
def pareto_frontier(d: pd.DataFrame, x: str, y: str) -> pd.DataFrame:
    """
    2D frontier: minimize x, maximize y.
    A simple scan works fine for our small grids.
    """
    d2 = d.sort_values(x, ascending=True).reset_index(drop=True)
    best = -1e18
    keep = []
    for i, row in d2.iterrows():
        if row[y] > best + 1e-12:
            keep.append(i)
            best = row[y]
    return d2.loc[keep]

# ---------------------------
# 9) Plot A: Pareto frontier
# ---------------------------
# ----------------------------
# 4) Visualization: Pareto Frontier (Blue-gradient coloring)
# ----------------------------
import matplotlib as mpl

PARETO_ATTACK = "clean"  # change to "syn0.3" / "del0.2" etc.
Y_COL = f"TPR@{int(TARGET_FPR*100)}%FPR"
plot_df = df[df["attack"] == PARETO_ATTACK].copy()

# Blue gradient by omega
cmap = mpl.cm.get_cmap("Blues")
omega_vals = plot_df["omega"].values.astype(float)
norm = mpl.colors.Normalize(vmin=float(np.min(omega_vals)), vmax=float(np.max(omega_vals)))

plt.figure(figsize=(12, 6))

method_names = plot_df["method"].unique().tolist()
markers = ["o", "s", "^", "D", "v", "P", "X"]

for mi, mname in enumerate(method_names):
    sub = plot_df[plot_df["method"] == mname].copy()

    # Scatter points colored by omega (blue gradient)
    point_colors = cmap(norm(sub["omega"].astype(float).values))
    plt.scatter(
        sub[PARETO_X], sub[Y_COL],
        c=point_colors,
        marker=markers[mi % len(markers)],
        s=70, alpha=0.85,
        edgecolors="none",
        label=mname
    )

    # Annotate (alpha,omega)
    for _, r in sub.iterrows():
        plt.annotate(
            f"={r['alpha']},={r['omega']}",
            (r[PARETO_X], r[Y_COL]),
            textcoords="offset points",
            xytext=(0, 8),
            ha="center",
            fontsize=8,
            alpha=0.9
        )

    # Pareto frontier line (also blue, but darker by using a high-omega color)
    front = pareto_frontier(sub, x_col=PARETO_X, y_col=Y_COL)
    line_color = cmap(0.85)  # dark-ish blue
    plt.plot(front[PARETO_X], front[Y_COL], linewidth=2.2, alpha=0.9, color=line_color)

# Reference line
plt.axhline(y=0.5, linestyle="--", alpha=0.4, color=cmap(0.6))

# Colorbar for omega
sm = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)
sm.set_array([])
cbar = plt.colorbar(sm, pad=0.01)
cbar.set_label("omega (frequency)  blue gradient", rotation=90)

plt.title(f"Pareto Frontier: {PARETO_ATTACK} (min {PARETO_X}) vs (max {Y_COL})", fontsize=14)
plt.xlabel(f"{PARETO_X} (lower is better)", fontsize=12)
plt.ylabel(f"{Y_COL} (higher is better)", fontsize=12)
plt.grid(True, alpha=0.25)

# Legend shows methods via marker shape
plt.legend(title="Method (marker)", loc="best")
plt.tight_layout()
plt.show()

# ---------------------------
# 10) Plot B: Robustness bars for one chosen alpha (or best point per method)
# ---------------------------
CHOSEN_ALPHA = strengths[len(strengths)//2]  # e.g., middle strength
sub = df[df["alpha"] == CHOSEN_ALPHA].copy()

if len(sub) > 0:
    plt.figure(figsize=(11, 5))
    # group by method; show best omega within this alpha
    best_rows = []
    for mname, g in sub.groupby("method"):
        # choose max Y_METRIC, tie-break by lower X_METRIC
        g2 = g.sort_values([Y_METRIC, X_METRIC], ascending=[False, True]).head(1)
        best_rows.append(g2)
    best = pd.concat(best_rows, ignore_index=True)

    x = np.arange(len(best))
    width = 0.35
    plt.bar(x - width/2, best["median_wm"], width, label="Median score (WM)", alpha=0.8)
    plt.bar(x + width/2, best["median_rob"], width, label=f"Median score after attack ({ROBUST_ATTACK_NAME})", alpha=0.8)
    plt.xticks(x, [f"{m}\n={o}" for m, o in zip(best["method"], best["omega"])])
    plt.ylabel("Detection statistic (median)")
    plt.title(f"Robustness snapshot at ={CHOSEN_ALPHA} (best  per method)")
    plt.grid(axis="y", alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.show()

# ---------------------------
# 11) Plot C: Score distributions for the single best point overall
# ---------------------------
best_overall = df.sort_values([Y_METRIC, X_METRIC], ascending=[False, True]).head(1).iloc[0]
print("Best overall setting:")
print(best_overall[["method","alpha","omega",Y_METRIC,X_METRIC,"median_wm","median_null","median_rob","AUC"]].to_dict())

# Re-run that best setting once to collect per-prompt score distributions (cheap: same prompts)
wm_cfg, sr_cfg = [m for m in METHODS if m["name"] == best_overall["method"]][0]["make_cfg"](best_overall["alpha"], best_overall["omega"])
null_cfg = GPWConfig(alpha=0.0, omega=wm_cfg.omega, salted=wm_cfg.salted, ctx_mode=wm_cfg.ctx_mode, ngram=wm_cfg.ngram)

wm_s, null_s = [], []
for i, p in enumerate(prompts):
    k_wm = hashlib.sha256(BASE_KEY + f"|wm|best|{i}".encode()).digest()
    k_pl = hashlib.sha256(BASE_KEY + f"|pl|best|{i}".encode()).digest()

    ids_pl, _ = gpw_generate_ids(p, k_pl, decode_cfg, null_cfg, sr_cfg)
    text_pl = tokenizer.decode(ids_pl[0], skip_special_tokens=True)
    null_s.append(detect_score(text_pl, k_pl, wm_cfg, sr_cfg))

    ids_wm, _ = gpw_generate_ids(p, k_wm, decode_cfg, wm_cfg, sr_cfg)
    text_wm = tokenizer.decode(ids_wm[0], skip_special_tokens=True)
    wm_s.append(detect_score(text_wm, k_wm, wm_cfg, sr_cfg))

plt.figure(figsize=(8, 5))
plt.hist(null_s, bins=12, alpha=0.6, label="Null (plain)", density=True)
plt.hist(wm_s, bins=12, alpha=0.6, label="Watermarked", density=True)
plt.axvline(best_overall["thr"], linestyle="--", label=f"thr@{int(TARGET_FPR*100)}%FPR")
plt.title("Score distribution (best setting)")
plt.xlabel("Detection statistic")
plt.ylabel("Density")
plt.legend()
plt.tight_layout()
plt.show()

print("Done.")

# @title  Fix Pareto Plot Crash + Better Frontier Plot (no cmap deprecation, no empty slices)
# Paste this cell AFTER your sweep cell that created `df`.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# --- Safety: ensure df exists and has expected columns ---
if "df" not in globals():
    raise RuntimeError("I can't find `df`. Run the sweep cell first (the one that builds the DataFrame).")

needed_cols = {"method","alpha","omega","delta_ppl","TPR@1%FPR","AUC"}
missing = sorted(list(needed_cols - set(df.columns)))
if missing:
    raise RuntimeError(f"`df` is missing columns: {missing}. Available: {list(df.columns)}")

# Pick axes (you can change these)
X_METRIC = "delta_ppl"    # lower is better
Y_METRIC = "TPR@1%FPR"    # higher is better (or use "AUC")

plot_df = df.copy()

# Drop any rows with NaNs in x/y
plot_df = plot_df.replace([np.inf, -np.inf], np.nan)
plot_df = plot_df.dropna(subset=[X_METRIC, Y_METRIC, "omega", "alpha", "method"])

if len(plot_df) == 0:
    raise RuntimeError("No rows left after cleaning NaNs. Check your sweep outputs for NaN PPL or scores.")

def pareto_frontier(d: pd.DataFrame, x: str, y: str) -> pd.DataFrame:
    """
    Pareto frontier for: minimize x, maximize y.
    """
    d2 = d.sort_values(x, ascending=True).reset_index(drop=True)
    best_y = -1e18
    keep_idx = []
    for i, row in d2.iterrows():
        if float(row[y]) > best_y + 1e-12:
            keep_idx.append(i)
            best_y = float(row[y])
    return d2.loc[keep_idx]

# ---------- Plot 1: Scatter + per-method frontier ----------
plt.figure(figsize=(12, 6))

# Use a stable colormap API (no deprecated get_cmap)
cmap = plt.colormaps["viridis"]

methods = sorted(plot_df["method"].unique().tolist())
markers = ["o", "s", "^", "D", "v", "P", "X"]

# For color: map omega to [0,1]
omega_vals = plot_df["omega"].astype(float).values
omin, omax = float(np.min(omega_vals)), float(np.max(omega_vals))
if abs(omax - omin) < 1e-12:
    # all omegas same; paint them mid-color
    omega_norm = np.ones_like(omega_vals) * 0.5
else:
    omega_norm = (omega_vals - omin) / (omax - omin)

# Plot each method separately
for mi, m in enumerate(methods):
    sub = plot_df[plot_df["method"] == m].copy()
    if len(sub) == 0:
        continue

    # scatter points
    sub_omega = sub["omega"].astype(float).values
    if abs(omax - omin) < 1e-12:
        colors = [cmap(0.5)] * len(sub)
    else:
        colors = [cmap((w - omin) / (omax - omin)) for w in sub_omega]

    plt.scatter(
        sub[X_METRIC].values,
        sub[Y_METRIC].values,
        marker=markers[mi % len(markers)],
        alpha=0.55,
        label=m,
        c=colors,
        edgecolors="none",
    )

    # frontier line
    front = pareto_frontier(sub, X_METRIC, Y_METRIC)
    if len(front) > 0:
        plt.plot(front[X_METRIC].values, front[Y_METRIC].values, linewidth=2)

        # annotate frontier points with (alpha, omega)
        for _, r in front.iterrows():
            plt.annotate(
                f"={r['alpha']}, ={r['omega']}",
                (float(r[X_METRIC]), float(r[Y_METRIC])),
                textcoords="offset points",
                xytext=(0, 8),
                ha="center",
                fontsize=8,
            )

plt.title(f"Pareto Frontier (min {X_METRIC}, max {Y_METRIC})")
plt.xlabel(f"{X_METRIC} (lower is better)")
plt.ylabel(f"{Y_METRIC} (higher is better)")
plt.grid(True, alpha=0.3)
plt.legend()
plt.tight_layout()
plt.show()

# ---------- Plot 2: Pick the best point overall and print it cleanly ----------
best = plot_df.sort_values([Y_METRIC, X_METRIC], ascending=[False, True]).head(1)
print("Best overall point (by max Y then min X):")
display(best[["method","alpha","omega",Y_METRIC,X_METRIC,"AUC","thr","median_wm","median_null","median_rob","ppl_wm","ppl_null","delta_ppl","n"]])

# ---------- Plot 3: Optional 3D-ish view (robustness vs quality vs detection) ----------
# Here robust axis uses median_rob (higher better). If missing, skip.
if "median_rob" in plot_df.columns:
    plt.figure(figsize=(12, 6))
    plt.scatter(plot_df[X_METRIC], plot_df[Y_METRIC], alpha=0.35)
    for _, r in best.iterrows():
        plt.scatter([r[X_METRIC]], [r[Y_METRIC]], s=180, marker="*", label="best")
    plt.title("Detection vs Quality (best point starred)")
    plt.xlabel(f"{X_METRIC} (lower better)")
    plt.ylabel(f"{Y_METRIC} (higher better)")
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.show()

# @title  Pareto Frontier (Perplexity vs TPR)  SR excluded  FIXED colorbar

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl

if "df" not in globals():
    raise RuntimeError("I can't find `df`. Run your sweep first (the one that creates the DataFrame).")

plot_df = df.copy()

# --- Ensure method exists ---
if "method" not in plot_df.columns:
    if "salted" in plot_df.columns and "SR" in plot_df.columns:
        plot_df["method"] = np.where(plot_df["salted"].astype(bool), "GPW-SP", "GPW")
        plot_df.loc[plot_df["SR"].astype(bool), "method"] = plot_df.loc[plot_df["SR"].astype(bool), "method"] + "+SR"
    else:
        # last resort: use variant if present
        if "variant" in plot_df.columns:
            plot_df["method"] = plot_df["variant"].astype(str)
        else:
            raise RuntimeError("No `method` column and cannot infer it (need salted/SR or variant).")

# --- Exclude SR rows (removes GPW-SP+SR and any SR variants) ---
if "SR" in plot_df.columns:
    plot_df = plot_df[plot_df["SR"].astype(bool) == False].copy()
else:
    plot_df = plot_df[~plot_df["method"].astype(str).str.contains(r"\+SR", regex=True)].copy()

# --- Axes to use ---
X = "ppl_wm"        # perplexity of WM text (lower better)
Y = "TPR@1%FPR"     # detection (higher better)

needed = {X, Y, "omega", "alpha", "method"}
missing = sorted(list(needed - set(plot_df.columns)))
if missing:
    raise RuntimeError(f"Missing columns: {missing}\nAvailable: {list(plot_df.columns)}")

# Clean data types
plot_df = plot_df.replace([np.inf, -np.inf], np.nan).dropna(subset=[X, Y, "omega", "alpha", "method"])
plot_df[X] = plot_df[X].astype(float)
plot_df[Y] = plot_df[Y].astype(float)
plot_df["omega"] = plot_df["omega"].astype(float)
plot_df["alpha"] = plot_df["alpha"].astype(float)

if len(plot_df) == 0:
    raise RuntimeError("No rows left after filtering/cleaning. Check NaNs and SR filtering.")

def pareto_frontier_minx_maxy(d: pd.DataFrame, x: str, y: str) -> pd.DataFrame:
    d2 = d.sort_values(x, ascending=True).reset_index(drop=True)
    best_y = -1e18
    keep = []
    for i, r in d2.iterrows():
        if float(r[y]) > best_y + 1e-12:
            keep.append(i)
            best_y = float(r[y])
    return d2.loc[keep].copy()

front = pareto_frontier_minx_maxy(plot_df, X, Y).sort_values(X, ascending=True).reset_index(drop=True)

# --- Print frontier table ---
cols_prefer = ["method","alpha","omega",Y,X,"AUC","thr","median_wm","median_null","median_rob","ppl_null","delta_ppl","n"]
cols = [c for c in cols_prefer if c in front.columns]
print("Pareto frontier points (SR excluded):")
display(front[cols])

# --- Plot with proper axes handle (fixes your colorbar error) ---
fig, ax = plt.subplots(figsize=(12, 6))

cmap = plt.colormaps["Blues"]  # bluish gradient
omin, omax = float(plot_df["omega"].min()), float(plot_df["omega"].max())

# If omega constant, don't bother with a colorbar.
if abs(omax - omin) < 1e-12:
    sc = ax.scatter(plot_df[X], plot_df[Y], alpha=0.65)
else:
    norm = mpl.colors.Normalize(vmin=omin, vmax=omax)
    sc = ax.scatter(plot_df[X], plot_df[Y], c=plot_df["omega"], cmap=cmap, norm=norm, alpha=0.7)
    cbar = fig.colorbar(sc, ax=ax)  # <-- key fix
    cbar.set_label(" (frequency)")

# Frontier line
ax.plot(front[X].values, front[Y].values, linewidth=2.5)

# Annotate frontier points
for _, r in front.iterrows():
    ax.annotate(
        f"{r['method']} ={r['alpha']:.2g}, ={r['omega']:.2g}",
        (float(r[X]), float(r[Y])),
        textcoords="offset points",
        xytext=(0, 8),
        ha="center",
        fontsize=8
    )

ax.set_title("Pareto Frontier (Perplexity vs TPR@1%FPR)  SR excluded")
ax.set_xlabel("Perplexity (watermarked) ")
ax.set_ylabel("TPR@1%FPR ")
ax.grid(True, alpha=0.3)

fig.tight_layout()
plt.show()

# @title  Pareto Frontier (Perplexity vs TPR@1%FPR)  label ALL points, SR excluded

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl

if "df" not in globals():
    raise RuntimeError("I can't find `df`. Run your sweep first (the one that creates the DataFrame).")

plot_df = df.copy()

# ---- Ensure method label exists ----
if "method" not in plot_df.columns:
    if "salted" in plot_df.columns and "SR" in plot_df.columns:
        plot_df["method"] = np.where(plot_df["salted"].astype(bool), "GPW-SP", "GPW")
        plot_df.loc[plot_df["SR"].astype(bool), "method"] = plot_df.loc[plot_df["SR"].astype(bool), "method"] + "+SR"
    elif "variant" in plot_df.columns:
        plot_df["method"] = plot_df["variant"].astype(str)
    else:
        raise RuntimeError("No `method` column and cannot infer one (need salted/SR or variant).")

# ---- Exclude SR ----
if "SR" in plot_df.columns:
    plot_df = plot_df[plot_df["SR"].astype(bool) == False].copy()
else:
    plot_df = plot_df[~plot_df["method"].astype(str).str.contains(r"\+SR", regex=True)].copy()

# ---- Build Perplexity if needed ----
if "delta_ppl" not in plot_df.columns:
    if "ppl_wm" in plot_df.columns and "ppl_null" in plot_df.columns:
        plot_df["delta_ppl"] = plot_df["ppl_wm"].astype(float) - plot_df["ppl_null"].astype(float)
    else:
        raise RuntimeError("Need delta_ppl OR (ppl_wm and ppl_null) columns to compute perplexity.")

# ---- Axes ----
X = "delta_ppl"     # PPL = ppl_wm - ppl_null (lower is better; 0 is ideal)
Y = "TPR@1%FPR"     # higher is better

needed = {X, Y, "omega", "alpha", "method"}
missing = sorted(list(needed - set(plot_df.columns)))
if missing:
    raise RuntimeError(f"Missing columns: {missing}\nAvailable: {list(plot_df.columns)}")

# ---- Clean + type-cast ----
plot_df = plot_df.replace([np.inf, -np.inf], np.nan).dropna(subset=[X, Y, "omega", "alpha", "method"])
plot_df[X] = plot_df[X].astype(float)
plot_df[Y] = plot_df[Y].astype(float)
plot_df["omega"] = plot_df["omega"].astype(float)
plot_df["alpha"] = plot_df["alpha"].astype(float)

if len(plot_df) == 0:
    raise RuntimeError("No rows left after filtering/cleaning. Check NaNs and SR filtering.")

# ---- Pareto frontier (min X, max Y) ----
def pareto_frontier_minx_maxy(d: pd.DataFrame, x: str, y: str) -> pd.DataFrame:
    d2 = d.sort_values(x, ascending=True).reset_index(drop=True)
    best_y = -1e18
    keep = []
    for i, r in d2.iterrows():
        if float(r[y]) > best_y + 1e-12:
            keep.append(i)
            best_y = float(r[y])
    return d2.loc[keep].copy()

front = pareto_frontier_minx_maxy(plot_df, X, Y).sort_values(X, ascending=True).reset_index(drop=True)

# ---- Print the frontier table ----
cols_prefer = ["method","alpha","omega",Y,X,"AUC","thr","median_wm","median_null","median_rob","ppl_wm","ppl_null","n","attack"]
cols = [c for c in cols_prefer if c in front.columns]
print("Pareto frontier points (SR excluded):")
display(front[cols])

# ---- Plot (blue gradient) ----
fig, ax = plt.subplots(figsize=(14, 7))
cmap = plt.colormaps["Blues"]

omin, omax = float(plot_df["omega"].min()), float(plot_df["omega"].max())
use_color = abs(omax - omin) >= 1e-12
norm = mpl.colors.Normalize(vmin=omin, vmax=omax) if use_color else None

# marker per method (so multiple methods stay readable)
methods = sorted(plot_df["method"].unique().tolist())
markers = ["o", "s", "^", "D", "v", "P", "X"]

for mi, m in enumerate(methods):
    sub = plot_df[plot_df["method"] == m].copy()
    if len(sub) == 0:
        continue
    if use_color:
        sc = ax.scatter(sub[X], sub[Y], c=sub["omega"], cmap=cmap, norm=norm,
                        alpha=0.75, marker=markers[mi % len(markers)], label=m)
    else:
        sc = ax.scatter(sub[X], sub[Y], alpha=0.75,
                        marker=markers[mi % len(markers)], label=m)

# ---- Label ALL points (with small offset pattern to reduce overlap) ----
# (This can get busy; but you explicitly asked for all labels.)
offsets = [(0,10), (10,0), (0,-10), (-10,0), (8,8), (8,-8), (-8,8), (-8,-8)]
for i, r in plot_df.reset_index(drop=True).iterrows():
    dx, dy = offsets[i % len(offsets)]
    ax.annotate(
        f"{r['method']} ={r['alpha']:.2g}, ={r['omega']:.2g}",
        (float(r[X]), float(r[Y])),
        textcoords="offset points",
        xytext=(dx, dy),
        ha="center",
        fontsize=8,
        bbox=dict(boxstyle="round,pad=0.2", fc="white", ec="none", alpha=0.6),
    )

# ---- Draw global frontier line ----
ax.plot(front[X].values, front[Y].values, linewidth=3)

# ---- Cosmetics ----
ax.set_title("Pareto Frontier (Perplexity vs TPR@1%FPR)  SR excluded")
ax.set_xlabel("Perplexity = PPL(WM)  PPL(plain)  (about 0 is ideal)")
ax.set_ylabel("TPR@1%FPR ")
ax.grid(True, alpha=0.3)
ax.legend(loc="lower right")

# ---- Colorbar ----
if use_color:
    cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)
    cbar.set_label(" (frequency)")

fig.tight_layout()
plt.show()

# @title  Pareto (Perplexity vs TPR@1%FPR)  CONNECT ALL CURVES (method), SR excluded

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl

if "df" not in globals():
    raise RuntimeError("I can't find `df`. Run your sweep first (the one that creates the DataFrame).")

plot_df = df.copy()

# ---- Ensure method label exists ----
if "method" not in plot_df.columns:
    if "salted" in plot_df.columns and "SR" in plot_df.columns:
        plot_df["method"] = np.where(plot_df["salted"].astype(bool), "GPW-SP", "GPW")
        plot_df.loc[plot_df["SR"].astype(bool), "method"] = plot_df.loc[plot_df["SR"].astype(bool), "method"] + "+SR"
    elif "variant" in plot_df.columns:
        plot_df["method"] = plot_df["variant"].astype(str)
    else:
        raise RuntimeError("No `method` column and cannot infer one (need salted/SR or variant).")

# ---- Exclude SR ----
if "SR" in plot_df.columns:
    plot_df = plot_df[plot_df["SR"].astype(bool) == False].copy()
else:
    plot_df = plot_df[~plot_df["method"].astype(str).str.contains(r"\+SR", regex=True)].copy()

# ---- Build Perplexity if needed ----
if "delta_ppl" not in plot_df.columns:
    if "ppl_wm" in plot_df.columns and "ppl_null" in plot_df.columns:
        plot_df["delta_ppl"] = plot_df["ppl_wm"].astype(float) - plot_df["ppl_null"].astype(float)
    else:
        raise RuntimeError("Need delta_ppl OR (ppl_wm and ppl_null) to compute perplexity.")

# ---- Axes ----
X = "delta_ppl"
Y = "TPR@1%FPR"
needed = {X, Y, "omega", "alpha", "method"}
missing = sorted(list(needed - set(plot_df.columns)))
if missing:
    raise RuntimeError(f"Missing columns: {missing}\nAvailable: {list(plot_df.columns)}")

# ---- Clean + cast ----
plot_df = plot_df.replace([np.inf, -np.inf], np.nan).dropna(subset=[X, Y, "omega", "alpha", "method"])
plot_df[X] = plot_df[X].astype(float)
plot_df[Y] = plot_df[Y].astype(float)
plot_df["omega"] = plot_df["omega"].astype(float)
plot_df["alpha"] = plot_df["alpha"].astype(float)
plot_df["method"] = plot_df["method"].astype(str)

if len(plot_df) == 0:
    raise RuntimeError("No rows left after filtering/cleaning.")

# ---- Pareto frontier (min X, max Y) ----
def pareto_frontier_minx_maxy(d: pd.DataFrame, x: str, y: str) -> pd.DataFrame:
    d2 = d.sort_values(x, ascending=True).reset_index(drop=True)
    best_y = -1e18
    keep = []
    for i, r in d2.iterrows():
        if float(r[y]) > best_y + 1e-12:
            keep.append(i)
            best_y = float(r[y])
    return d2.loc[keep].copy()

front = pareto_frontier_minx_maxy(plot_df, X, Y).sort_values(X, ascending=True).reset_index(drop=True)

# ---- Plot ----
fig, ax = plt.subplots(figsize=(14, 7))
cmap = plt.colormaps["Blues"]
omin, omax = float(plot_df["omega"].min()), float(plot_df["omega"].max())
use_color = abs(omax - omin) >= 1e-12
norm = mpl.colors.Normalize(vmin=omin, vmax=omax) if use_color else None

# Distinct marker per method
methods = sorted(plot_df["method"].unique().tolist())
markers = ["o", "s", "^", "D", "v", "P", "X"]

# ---- Connect EVERYTHING: lines for each (method, omega), sorted by alpha ----
for mi, method in enumerate(methods):
    mdf = plot_df[plot_df["method"] == method]
    for omega_val, g in mdf.groupby("omega"):
        g2 = g.sort_values("alpha", ascending=True)
        # color by omega
        if use_color:
            line_color = cmap(norm(float(omega_val)))
        else:
            line_color = None

        ax.plot(
            g2[X].values, g2[Y].values,
            linewidth=2.0,
            alpha=0.85,
            color=line_color,
        )
        # scatter points on top
        ax.scatter(
            g2[X].values, g2[Y].values,
            alpha=0.85,
            marker=markers[mi % len(markers)],
            c=(g2["omega"].values if use_color else None),
            cmap=(cmap if use_color else None),
            norm=(norm if use_color else None),
            label=None
        )

# ---- Legend (methods only) ----
# Create dummy handles so legend isn't spammed by many groups
for mi, method in enumerate(methods):
    ax.scatter([], [], marker=markers[mi % len(markers)], label=method, color="black", alpha=0.8)

# ---- Overlay global Pareto frontier thicker ----
ax.plot(front[X].values, front[Y].values, linewidth=3.5)

# ---- Labels (optional: label all points; can get busy) ----
offsets = [(0,10), (10,0), (0,-10), (-10,0), (8,8), (8,-8), (-8,8), (-8,-8)]
for i, r in plot_df.reset_index(drop=True).iterrows():
    dx, dy = offsets[i % len(offsets)]
    ax.annotate(
        f"{r['method']} ={r['alpha']:.2g}, ={r['omega']:.2g}",
        (float(r[X]), float(r[Y])),
        textcoords="offset points",
        xytext=(dx, dy),
        ha="center",
        fontsize=8,
        bbox=dict(boxstyle="round,pad=0.2", fc="white", ec="none", alpha=0.55),
    )

ax.set_title("Connected Pareto Plot (Perplexity vs TPR@1%FPR)  SR excluded")
ax.set_xlabel("Perplexity = PPL(WM)  PPL(plain)    (0 is ideal)")
ax.set_ylabel("TPR@1%FPR  ")
ax.grid(True, alpha=0.3)
ax.legend(loc="lower right", title="method")

# ---- Colorbar (omega) ----
if use_color:
    cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)
    cbar.set_label(" (frequency)")

fig.tight_layout()
plt.show()

print(df)